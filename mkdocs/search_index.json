{
    "docs": [
        {
            "location": "/", 
            "text": "DataRescue Workflow\n\n\nThis document describes DataRescue activities both at in-person events and remotely, as developed by the \nDataRefuge project\n and \nEDGI\n. It explains the process that a URL/dataset goes through from the time it has been identified, either by a \nSeeder\n as difficult to preserve, or  \"uncrawlable,\" until it is made available as a record in the \ndatarefuge.org\n data catalog. The process involves several stages and is designed to maximize smooth hand-offs. At each step the data is with someone with distinct expertise and the data is always being tracked for security.\n\n\nEvent Organizers\n\n\n\n\nLearn about what you need to do to \nbefore an event\n.\n\n\n\n\nEvent Attendees\n\n\n\n\nCreate an account on the \nDataRefuge Slack\n or use the Slack team recommended by event organizers. This is where people share expertise and answer each other's questions.  \n\n\nPick your role from below, get account credentials, and make sure you have access to the key documents and tools you need to work. Organizers will instruct you on these steps.\n\n\nGo over the workflow documentation below and read the relevant sections(s).\n\n\n\n\n\n\nOverview\n\n\n1. \nSeeding\n\n\nSeeders canvass the resources of a given government agency, identifying important URLs. They identify whether those URLs can be crawled by the Internet Archive's webcrawler. If the URLs are crawlable, the Seeders nominate them to the End-of-Term (EOT) project. Otherwise, they add them to the Archivers app using the project's Chrome Extension.\n\n\n2. \nResearching\n\n\nResearchers inspect the \"uncrawlable\" list to confirm that Seeders' assessments were correct (that is, that the URL/dataset is indeed uncrawlable), and investigate how the dataset could be best harvested. \nResearching.md\n describes this process in more detail.\n\n\nWe recommend that Researchers and Harvesters (see below) work together in pairs, as much communication is needed between the two roles. In some cases, one person will fulfill both roles.\n\n\n3. \nHarvesting\n\n\nHarvesters take the \"uncrawlable\" data and try to figure out how to actually capture it based on the recommendations of the Researchers. This is a complex task which can require substantial technical expertise and different techniques for different tasks. Harvesters should also review the \nHarvesting Toolkit\n for tools.\n\n\n4. Checking\n\n\nNote: This role is currently performed by the Baggers, and does not exist separately.\n\n\nCheckers inspect a harvested dataset and make sure that it is complete. The main question the checkers need to answer is \"will the bag make sense to a scientist\"? Checkers need to have an in-depth understanding of harvesting goals and potential content variations for datasets.\n\n\n5. \nBagging\n\n\nBaggers perform some quality assurance on the dataset to make sure the content is correct and corresponds to the original URL. Then they package the data into a bagit file (or \"bag\"), which includes basic technical metadata, and upload it to the final DataRefuge destination.\n\n\n6. \nDescribing\n\n\nDescribers create a descriptive record in the DataRefuge CKAN repository for each bag. Then they link the record to the bag and make the record public.\n\n\n\n\nPartners\n\n\nDataRescue is a broad, grassroots effort with support from numerous local and nationwide networks. \nDataRefuge\n and \nEDGI\n partner with local organizers in supporting these events. See more of our institutional partners on the \nDataRefuge home page\n.", 
            "title": "Home"
        }, 
        {
            "location": "/#datarescue-workflow", 
            "text": "This document describes DataRescue activities both at in-person events and remotely, as developed by the  DataRefuge project  and  EDGI . It explains the process that a URL/dataset goes through from the time it has been identified, either by a  Seeder  as difficult to preserve, or  \"uncrawlable,\" until it is made available as a record in the  datarefuge.org  data catalog. The process involves several stages and is designed to maximize smooth hand-offs. At each step the data is with someone with distinct expertise and the data is always being tracked for security.", 
            "title": "DataRescue Workflow"
        }, 
        {
            "location": "/#event-organizers", 
            "text": "Learn about what you need to do to  before an event .", 
            "title": "Event Organizers"
        }, 
        {
            "location": "/#event-attendees", 
            "text": "Create an account on the  DataRefuge Slack  or use the Slack team recommended by event organizers. This is where people share expertise and answer each other's questions.    Pick your role from below, get account credentials, and make sure you have access to the key documents and tools you need to work. Organizers will instruct you on these steps.  Go over the workflow documentation below and read the relevant sections(s).", 
            "title": "Event Attendees"
        }, 
        {
            "location": "/#overview", 
            "text": "", 
            "title": "Overview"
        }, 
        {
            "location": "/#1-seeding", 
            "text": "Seeders canvass the resources of a given government agency, identifying important URLs. They identify whether those URLs can be crawled by the Internet Archive's webcrawler. If the URLs are crawlable, the Seeders nominate them to the End-of-Term (EOT) project. Otherwise, they add them to the Archivers app using the project's Chrome Extension.", 
            "title": "1. Seeding"
        }, 
        {
            "location": "/#2-researching", 
            "text": "Researchers inspect the \"uncrawlable\" list to confirm that Seeders' assessments were correct (that is, that the URL/dataset is indeed uncrawlable), and investigate how the dataset could be best harvested.  Researching.md  describes this process in more detail.  We recommend that Researchers and Harvesters (see below) work together in pairs, as much communication is needed between the two roles. In some cases, one person will fulfill both roles.", 
            "title": "2. Researching"
        }, 
        {
            "location": "/#3-harvesting", 
            "text": "Harvesters take the \"uncrawlable\" data and try to figure out how to actually capture it based on the recommendations of the Researchers. This is a complex task which can require substantial technical expertise and different techniques for different tasks. Harvesters should also review the  Harvesting Toolkit  for tools.", 
            "title": "3. Harvesting"
        }, 
        {
            "location": "/#4-checking", 
            "text": "Note: This role is currently performed by the Baggers, and does not exist separately.  Checkers inspect a harvested dataset and make sure that it is complete. The main question the checkers need to answer is \"will the bag make sense to a scientist\"? Checkers need to have an in-depth understanding of harvesting goals and potential content variations for datasets.", 
            "title": "4. Checking"
        }, 
        {
            "location": "/#5-bagging", 
            "text": "Baggers perform some quality assurance on the dataset to make sure the content is correct and corresponds to the original URL. Then they package the data into a bagit file (or \"bag\"), which includes basic technical metadata, and upload it to the final DataRefuge destination.", 
            "title": "5. Bagging"
        }, 
        {
            "location": "/#6-describing", 
            "text": "Describers create a descriptive record in the DataRefuge CKAN repository for each bag. Then they link the record to the bag and make the record public.", 
            "title": "6. Describing"
        }, 
        {
            "location": "/#partners", 
            "text": "DataRescue is a broad, grassroots effort with support from numerous local and nationwide networks.  DataRefuge  and  EDGI  partner with local organizers in supporting these events. See more of our institutional partners on the  DataRefuge home page .", 
            "title": "Partners"
        }, 
        {
            "location": "/logistics-procedures/", 
            "text": "Logistics and Procedures\n\n\nThis document contains Yale-specific information about the event. Some of this information comes from the \nDataRescueNHV page\n.\n\n\nBasic Event Logistics\n\n\nBathrooms\n\n\nThere are several restrooms on the first floor. From the TEAL classroom, take a left, another left, and yet another left. The restrooms will be ... on your left. There are additional bathrooms downstairs.\n\n\nYale prohibits discrimination based on gender identity or expression. Community members and campus guests are free to use the bathroom of the gender with which they identify. \nAll-gender restrooms are available using this map.\n\n\nWhat You Need\n\n\nYou will need, at minimum, a laptop. If your laptop doesn't have an ethernet port,  please brining your own ethernet adapter, if you have one. We\u2019ll provide cables and external hard drives. We\u2019ll provide beverages, lunch, and snacks!\n\n\nCode of Conduct\n\n\nThe organizers of #DataRescueNHV ask for your assistance to make this event welcoming, inclusive, and harassment-free for all participants. Please review our \ncode of conduct\n and talk to an event organizer if you have any questions or concerns.\n\n\nTechnology and Communication Logistics\n\n\nUSB Drives, Accounts, and Internet\n\n\nIf you are a Yale affiliate, we recommend using YaleSecure.\n\n\nIf you are on a track, such as Harvester, Bagger, or Checker, that requires downloading and uploading files, we recommend using Ethernet.\n\n\nIf you need a USB drive or another storage solution, please find Joshua Dull (Slack: @joshua.dull).\n\n\nIf you need accounts, please find Scott Matheson (@smatheson) or talk to Joshua Dull (@joshua.dull) or Kayleigh Boh\u00e9mier (@kayebohemier).\n\n\nSlack Communication\n\n\nThe Slack channels include special places for all of the roles described above for specific troubleshooting circumstances. You can also privately message individuals using the Slack. If you need assistance setting up Slack so you can see everything you need, please find Kayleigh Boh\u00e9mier, who is @kayebohemier on Slack.\n\n\nProblems with Data Harvesting?\n\n\nSometimes, data will be difficult or not feasible for you to harvest. In those cases, please follow this process:\n\n\n\n\nAsk for assistance from 2 other Harvesters or Researchers to verify that the data is not harvestable. If it is GIS data, please see Miriam Olivares.\n\n\nAdd the unique ID for the dataset \nto the FOIA request form\n, along with any additional information you have. (This form also means \"contact agency\" \u2014 it won't always lead to a FOIA request.)\n\n\nMake a note in the archivers.space app to let everyone know that this data is not harvestable.\n\n\n\n\nAgency and Data Selection Priorities\n\n\nIf you're worried about specific datasets, these data likely come from the EPA or NOAA if they are environmental data. Other datasets come from places like the Department of Justice or Department of Education. The EPA and NOAA were the highest-priority agencies at the beginning of the #DataRescue project and have already been covered by other institutions.\n\n\nCurrently, the process for selecting agencies and departments for #DataRescue depends on two things: What has already been covered and what needs a primer. A \nprimer\n is a basic description of the units within a Department (e.g., the Department of Justice), along with its agencies and sub-agencies. These primers are used by \nseeders and sorters\n to check the subagency web sites for uncrawlable data. If a Department has no primer, or if it has limited agency and subagency information in its primer, it isn't ready for seeders and sorters to use. Some individuals at the event on Saturday will be creating primers to seed future events at other institutions. If you have concerns about social science data in particular, writing a primer on Saturday is a good way to ensure that something gets done at future events..\n\n\nData Rescue is focusing on identifying \nnew\n datasets. Long-term preservation is a future phase of the project, but \nthere are already conversations like this one\n about what that might look like.", 
            "title": "New Haven-Specific"
        }, 
        {
            "location": "/logistics-procedures/#logistics-and-procedures", 
            "text": "This document contains Yale-specific information about the event. Some of this information comes from the  DataRescueNHV page .", 
            "title": "Logistics and Procedures"
        }, 
        {
            "location": "/logistics-procedures/#basic-event-logistics", 
            "text": "", 
            "title": "Basic Event Logistics"
        }, 
        {
            "location": "/logistics-procedures/#bathrooms", 
            "text": "There are several restrooms on the first floor. From the TEAL classroom, take a left, another left, and yet another left. The restrooms will be ... on your left. There are additional bathrooms downstairs.  Yale prohibits discrimination based on gender identity or expression. Community members and campus guests are free to use the bathroom of the gender with which they identify.  All-gender restrooms are available using this map.", 
            "title": "Bathrooms"
        }, 
        {
            "location": "/logistics-procedures/#what-you-need", 
            "text": "You will need, at minimum, a laptop. If your laptop doesn't have an ethernet port,  please brining your own ethernet adapter, if you have one. We\u2019ll provide cables and external hard drives. We\u2019ll provide beverages, lunch, and snacks!", 
            "title": "What You Need"
        }, 
        {
            "location": "/logistics-procedures/#code-of-conduct", 
            "text": "The organizers of #DataRescueNHV ask for your assistance to make this event welcoming, inclusive, and harassment-free for all participants. Please review our  code of conduct  and talk to an event organizer if you have any questions or concerns.", 
            "title": "Code of Conduct"
        }, 
        {
            "location": "/logistics-procedures/#technology-and-communication-logistics", 
            "text": "", 
            "title": "Technology and Communication Logistics"
        }, 
        {
            "location": "/logistics-procedures/#usb-drives-accounts-and-internet", 
            "text": "If you are a Yale affiliate, we recommend using YaleSecure.  If you are on a track, such as Harvester, Bagger, or Checker, that requires downloading and uploading files, we recommend using Ethernet.  If you need a USB drive or another storage solution, please find Joshua Dull (Slack: @joshua.dull).  If you need accounts, please find Scott Matheson (@smatheson) or talk to Joshua Dull (@joshua.dull) or Kayleigh Boh\u00e9mier (@kayebohemier).", 
            "title": "USB Drives, Accounts, and Internet"
        }, 
        {
            "location": "/logistics-procedures/#slack-communication", 
            "text": "The Slack channels include special places for all of the roles described above for specific troubleshooting circumstances. You can also privately message individuals using the Slack. If you need assistance setting up Slack so you can see everything you need, please find Kayleigh Boh\u00e9mier, who is @kayebohemier on Slack.", 
            "title": "Slack Communication"
        }, 
        {
            "location": "/logistics-procedures/#problems-with-data-harvesting", 
            "text": "Sometimes, data will be difficult or not feasible for you to harvest. In those cases, please follow this process:   Ask for assistance from 2 other Harvesters or Researchers to verify that the data is not harvestable. If it is GIS data, please see Miriam Olivares.  Add the unique ID for the dataset  to the FOIA request form , along with any additional information you have. (This form also means \"contact agency\" \u2014 it won't always lead to a FOIA request.)  Make a note in the archivers.space app to let everyone know that this data is not harvestable.", 
            "title": "Problems with Data Harvesting?"
        }, 
        {
            "location": "/logistics-procedures/#agency-and-data-selection-priorities", 
            "text": "If you're worried about specific datasets, these data likely come from the EPA or NOAA if they are environmental data. Other datasets come from places like the Department of Justice or Department of Education. The EPA and NOAA were the highest-priority agencies at the beginning of the #DataRescue project and have already been covered by other institutions.  Currently, the process for selecting agencies and departments for #DataRescue depends on two things: What has already been covered and what needs a primer. A  primer  is a basic description of the units within a Department (e.g., the Department of Justice), along with its agencies and sub-agencies. These primers are used by  seeders and sorters  to check the subagency web sites for uncrawlable data. If a Department has no primer, or if it has limited agency and subagency information in its primer, it isn't ready for seeders and sorters to use. Some individuals at the event on Saturday will be creating primers to seed future events at other institutions. If you have concerns about social science data in particular, writing a primer on Saturday is a good way to ensure that something gets done at future events..  Data Rescue is focusing on identifying  new  datasets. Long-term preservation is a future phase of the project, but  there are already conversations like this one  about what that might look like.", 
            "title": "Agency and Data Selection Priorities"
        }, 
        {
            "location": "/self-sorting/", 
            "text": "A Quick Guide to Self-Sorting\n\n\nWe will be updating this document with information about our team leaders!\n\n\nFor any questions about GIS information from Researchers or Harvesters, please see Miriam Olivares (@molivares).\n\n\nSurveyors\n\n\n\n\nKnowledge of government resources\n\n\nFamiliar with government web site structure\n\n\nSee more on \nthis guide\n\n\n\n\nTeam leader:\n\n\nSeeders/Sorters\n\n\n\n\nBasic Internet browsing\n\n\n\n\nTeam leader:\n\n\nResearchers\n\n\n\n\nBasic Internet browsing\n\n\nFront end web experience (any HTML, CSS, and/or content management)\n\n\nDisciplinary expertise\n\n\n\n\nTeam leader: Limor Peer\nGIS leader: Miriam Olivares\n\n\nHarvesters\n\n\n\n\nFront end web experience\n\n\nBackend web experience\n\n\nData formats\n\n\nAPIs and/or web scraping\n\n\nProgramming language(s)\n\n\n\n\nTeam leader: Steve Wieda\nGIS leader: Miriam Olivares\n\n\nBaggers/Checkers\n\n\n\n\nData formats (e.g., Excel, csv, GIS data, \nc.)\n\n\nDisciplinary expertise\n\n\n(Some) metadata\n\n\n\n\nTeam leader:\n\n\nDescribers\n\n\n\n\nMetadata\n\n\nData formats\n\n\n\n\nTeam leader: Julie Linden\n\n\nStorytellers\n\n\n\n\nSocial media\n\n\n\n\nTeam leaders: Marie Garambois and Melanie Maksin", 
            "title": "Where Do I Go?"
        }, 
        {
            "location": "/self-sorting/#a-quick-guide-to-self-sorting", 
            "text": "We will be updating this document with information about our team leaders!  For any questions about GIS information from Researchers or Harvesters, please see Miriam Olivares (@molivares).", 
            "title": "A Quick Guide to Self-Sorting"
        }, 
        {
            "location": "/self-sorting/#surveyors", 
            "text": "Knowledge of government resources  Familiar with government web site structure  See more on  this guide   Team leader:", 
            "title": "Surveyors"
        }, 
        {
            "location": "/self-sorting/#seederssorters", 
            "text": "Basic Internet browsing   Team leader:", 
            "title": "Seeders/Sorters"
        }, 
        {
            "location": "/self-sorting/#researchers", 
            "text": "Basic Internet browsing  Front end web experience (any HTML, CSS, and/or content management)  Disciplinary expertise   Team leader: Limor Peer\nGIS leader: Miriam Olivares", 
            "title": "Researchers"
        }, 
        {
            "location": "/self-sorting/#harvesters", 
            "text": "Front end web experience  Backend web experience  Data formats  APIs and/or web scraping  Programming language(s)   Team leader: Steve Wieda\nGIS leader: Miriam Olivares", 
            "title": "Harvesters"
        }, 
        {
            "location": "/self-sorting/#baggerscheckers", 
            "text": "Data formats (e.g., Excel, csv, GIS data,  c.)  Disciplinary expertise  (Some) metadata   Team leader:", 
            "title": "Baggers/Checkers"
        }, 
        {
            "location": "/self-sorting/#describers", 
            "text": "Metadata  Data formats   Team leader: Julie Linden", 
            "title": "Describers"
        }, 
        {
            "location": "/self-sorting/#storytellers", 
            "text": "Social media   Team leaders: Marie Garambois and Melanie Maksin", 
            "title": "Storytellers"
        }, 
        {
            "location": "/advance-work/", 
            "text": "Organizing an Event\n\n\nThis document is meant for DataRescue event organizers. There are lots of ways to prepare for an event. This document highlights only the technical aspects of preparation. There are lots of logistical and other aspects to look out for as well, but this is the minimum needed to use the workflow we propose.\n\n\nNote that after an event, participants might want to continue the work remotely, and our workflow is designed to make that possible.\n\n\nBefore starting, your team should go through the following steps.\n\n HEAD:advance-work.md\n=======\n\n\nThe Basics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndatarefuge/master:docs/advance-work.md\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRead through the entire workflow documentation\n\n HEAD:advance-work.md\n\n\nSign up for the datarefuge slack and make sure there's a channel for your event.\n\n\nDefine your teams. They are usually: Seeders/Sorters, Researchers, Harvesters, Checkers, Baggers, and Describers. Although in some cases, some of the roles can be conflated.\n\n\nIn particular, we recommend that Researchers and Harvesters work very closely with each other, for instance in pairs or in small groups. In some cases, a single person might be both a Researcher and a Harvester.\n\n\n\n\nEach team should have team leaders, aka \"guides\".\n\n\n\n\nSign up for the datarefuge slack and make sure there's a channel for your event\n\n\nDefine your teams. They are usually: Seeders/Sorters, Researchers, Harvesters, Checkers, Baggers, and Describers. Although in some cases, some of the roles can be conflated\n\n\nIn particular, we recommend that Researchers and Harvesters work very closely with each other, for instance in pairs or in small groups. In some cases, a single person might be both a Researcher and a Harvester.\n\n\nEach team should have team leaders, aka \"guides\".\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndatarefuge/master:docs/advance-work.md\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe event organizers and team leaders should schedule a call with DataRefuge to go over the process.\n\n\nThe event organizers and team leaders for the Seeders and Sorters should also check in with EDGI folks for info about how to make sure that you're seeding and sorting effectively.\n\n\n\n\nNote that the Describers role is being redeveloped at the moment, so it is currently not enabled.\n\n\nPrimer and sub-primer documents\n\n\n\n\nMake sure your event has its designated primer and sub-primer documents\n\n\nThose are documents that will inform the work of the Seeders/Sorters at your event. They will tell them which website or website sections they should be focusing on for URL discovery.\n\n\nAn EDGI coordinator will setup these documents for you.\n\n\n\n\nArchivers app\n\n\n\n\nThe \nArchivers app\n enables us to collectively keep track of all the archiving work being done.\n\n\nIt will also help coordinate the work of different roles (Researchers, Harvesters, Checkers, Baggers, Describers) on each URL.\n\n\n\n\n\n\nThe app will include URLs coming from two main sources:\n\n HEAD:advance-work.md\n\n\nURLs that were nominated by Seeders at a previous DataRescue event,\n\n\n\n\nURLs that were identified througth the Union of Concerned Scientists survey, which asked the scientific community to list the most vulnerable and important data currently on accessible through federal websites.\n\n\n\n\n\n\nYou need to make sure that:\n\n\n\n\nYour event is listed in the app.\n\n\nTalk to the DataRefuge organizers about this.\n\n\n\n\n\n\nAll the event participants who need it have access to the app (see Credentials section below)\n\n\n\n\nCrawl vs. Harvest: storage location\n\n\n- The main triage point of the workflow is whether a URL can be automatically crawled, for instance by the Internet Archive, or whether it  needs to be manually harvested.\n\n\n- URLs that were nominated by Seeders at a previous DataRescue event,\n- URLs that were identified through the Union of Concerned Scientists survey, which asked the scientific community to list the most vulnerable and important data currently on accessible through federal websites\n\n\n\n\n\nYou need to make sure that:\n\n\nYour event is listed in the app, talk to the DataRefuge organizers about this\n\n\nAll the event participants who need it have access to the app (see Credentials section below)\n\n\n\n\n\n\n\n\nCrawl vs. Harvest: storage location\n\n\n\n\nThe main triage point of the workflow is whether a URL can be automatically crawled, for instance by the Internet Archive, or whether it needs to be manually harvested.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndatarefuge/master:docs/advance-work.md\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe crawling process does not require any separate storage management, as the crawlable URLs are nominated to the Internet Archive, who will take care of the actual file storage after they have crawled the pages. See the \nSeeders/Sorters documentation\n for more information on this process.\n\n\nThe datasets harvested through the harvest process and uploaded through are the Archivers app are stored on S3 storage managed by DataRefuge.\n\n\nAt this time there is no direct access to the files stored on S3 for security reason.\n\n\n\n\n\n\n\n HEAD:advance-work.md\n\n\n\nCredentials\n\n\n\n\nThe Researchers/Harvesters/Checkers/Baggers need to have an account on the \nArchivers app\n\n\nYou will need to generate invites for each one \nwithin the app\n, and paste the URL generated in a slack Direct Message or an email.\n\n\nEach participant invited will automatically \"belongs\" to your event in the app.\n\n\nCheckers and Baggers need to be given explicit privileges in the app to have access to the Checking (i.e. \"Finalize\") and Bagging sections.\n\n\nSeeders/Sorters do not need access to the Archivers app.\n\n\n=======\n\n!-- - You need two S3 \"buckets\" (i.e., directories) for your harvested files.\n\n\nThe Harvesters will upload the files they harvest to the first bucket (\"pre-bag\" bucket)\n\n\nIn some cases, the Checkers will also upload improved versions of the files to the same pre-bag bucket\n\n\nThe Baggers will turn those files into bags and upload the bags to the second bucket (\"bag\" bucket)\n\n\n\n\n\n\nA DataRefuge coordinator will set up the two S3 buckets for you\n\n\nThey will also make sure that the Uploader web applications used by Harvesters, Checkers, and Baggers \"knows\" about your event and has the name of your event listed as an option.\n\n\nMost uploads to S3 will be done through the web-based Uploader application (http://drp-upload.herokuapp.com/ or http://drp-upload-bagger.herokuapp.com/). This application can handle files up to 5 Gigs.\n\n\n\n\n\n\nOne person at the event should be designated as the S3 System Administrator and will have direct access to the S3 buckets for the event. The S3 Sys Admin should be someone with advanced technical skills and will be responsible for 2 things:\n\n\nUpload very large sets (beyond 5 Gigs) through an alternate method (provided by DataRefuge)\n\n\nKeep the buckets cleaned up and organized.\n\n\n\n\n\n\nNote that large files Uploaders need not be coders, but they should have a little experience working in command line, and computers with python 2.7.--\n\n\n\n\nCredentials\n\n\n\n\nThe Researchers/Harvesters/Checkers/Baggers need to have an account on the \nArchivers app\n\n\nYou will need to generate invites for each one \nwithin the app\n, and paste the URL generated in a slack Direct Message or an email\n\n\nEach participant invited will automatically \"belongs\" to your event in the app\n\n\n\n\n\n\nCheckers and Baggers need to be given explicit privileges in the app to have access to the Checking (i.e. \"Finalize\") and Bagging sections\n\n\nSeeders/Sorters do not need access to the Archivers app\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndatarefuge/master:docs/advance-work.md\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOther supplies\n\n\nMake sure you have a few thumb drives to handle very large data sets (above 5 Gigs).\n\n\nAfter the event\n\n\n\n\nParticipants might want to continue the work started at the event remotely\n\n\nThis should be possible, as our workflow is meant to function in-person as well as remotely", 
            "title": "Advance Work"
        }, 
        {
            "location": "/advance-work/#organizing-an-event", 
            "text": "This document is meant for DataRescue event organizers. There are lots of ways to prepare for an event. This document highlights only the technical aspects of preparation. There are lots of logistical and other aspects to look out for as well, but this is the minimum needed to use the workflow we propose.  Note that after an event, participants might want to continue the work remotely, and our workflow is designed to make that possible.  Before starting, your team should go through the following steps.  HEAD:advance-work.md\n=======", 
            "title": "Organizing an Event"
        }, 
        {
            "location": "/advance-work/#the-basics", 
            "text": "datarefuge/master:docs/advance-work.md          Read through the entire workflow documentation  HEAD:advance-work.md  Sign up for the datarefuge slack and make sure there's a channel for your event.  Define your teams. They are usually: Seeders/Sorters, Researchers, Harvesters, Checkers, Baggers, and Describers. Although in some cases, some of the roles can be conflated.  In particular, we recommend that Researchers and Harvesters work very closely with each other, for instance in pairs or in small groups. In some cases, a single person might be both a Researcher and a Harvester.", 
            "title": "The Basics"
        }, 
        {
            "location": "/advance-work/#each-team-should-have-team-leaders-aka-guides", 
            "text": "Sign up for the datarefuge slack and make sure there's a channel for your event  Define your teams. They are usually: Seeders/Sorters, Researchers, Harvesters, Checkers, Baggers, and Describers. Although in some cases, some of the roles can be conflated  In particular, we recommend that Researchers and Harvesters work very closely with each other, for instance in pairs or in small groups. In some cases, a single person might be both a Researcher and a Harvester.  Each team should have team leaders, aka \"guides\".        datarefuge/master:docs/advance-work.md            The event organizers and team leaders should schedule a call with DataRefuge to go over the process.  The event organizers and team leaders for the Seeders and Sorters should also check in with EDGI folks for info about how to make sure that you're seeding and sorting effectively.   Note that the Describers role is being redeveloped at the moment, so it is currently not enabled.", 
            "title": "Each team should have team leaders, aka \"guides\"."
        }, 
        {
            "location": "/advance-work/#primer-and-sub-primer-documents", 
            "text": "Make sure your event has its designated primer and sub-primer documents  Those are documents that will inform the work of the Seeders/Sorters at your event. They will tell them which website or website sections they should be focusing on for URL discovery.  An EDGI coordinator will setup these documents for you.", 
            "title": "Primer and sub-primer documents"
        }, 
        {
            "location": "/advance-work/#archivers-app", 
            "text": "The  Archivers app  enables us to collectively keep track of all the archiving work being done.  It will also help coordinate the work of different roles (Researchers, Harvesters, Checkers, Baggers, Describers) on each URL.    The app will include URLs coming from two main sources:  HEAD:advance-work.md  URLs that were nominated by Seeders at a previous DataRescue event,   URLs that were identified througth the Union of Concerned Scientists survey, which asked the scientific community to list the most vulnerable and important data currently on accessible through federal websites.    You need to make sure that:   Your event is listed in the app.  Talk to the DataRefuge organizers about this.    All the event participants who need it have access to the app (see Credentials section below)", 
            "title": "Archivers app"
        }, 
        {
            "location": "/advance-work/#crawl-vs-harvest-storage-location", 
            "text": "", 
            "title": "Crawl vs. Harvest: storage location"
        }, 
        {
            "location": "/advance-work/#-the-main-triage-point-of-the-workflow-is-whether-a-url-can-be-automatically-crawled-for-instance-by-the-internet-archive-or-whether-it-needs-to-be-manually-harvested", 
            "text": "- URLs that were nominated by Seeders at a previous DataRescue event,\n- URLs that were identified through the Union of Concerned Scientists survey, which asked the scientific community to list the most vulnerable and important data currently on accessible through federal websites   You need to make sure that:  Your event is listed in the app, talk to the DataRefuge organizers about this  All the event participants who need it have access to the app (see Credentials section below)", 
            "title": "- The main triage point of the workflow is whether a URL can be automatically crawled, for instance by the Internet Archive, or whether it  needs to be manually harvested."
        }, 
        {
            "location": "/advance-work/#crawl-vs-harvest-storage-location_1", 
            "text": "The main triage point of the workflow is whether a URL can be automatically crawled, for instance by the Internet Archive, or whether it needs to be manually harvested.        datarefuge/master:docs/advance-work.md          The crawling process does not require any separate storage management, as the crawlable URLs are nominated to the Internet Archive, who will take care of the actual file storage after they have crawled the pages. See the  Seeders/Sorters documentation  for more information on this process.  The datasets harvested through the harvest process and uploaded through are the Archivers app are stored on S3 storage managed by DataRefuge.  At this time there is no direct access to the files stored on S3 for security reason.     HEAD:advance-work.md", 
            "title": "Crawl vs. Harvest: storage location"
        }, 
        {
            "location": "/advance-work/#credentials", 
            "text": "The Researchers/Harvesters/Checkers/Baggers need to have an account on the  Archivers app  You will need to generate invites for each one  within the app , and paste the URL generated in a slack Direct Message or an email.  Each participant invited will automatically \"belongs\" to your event in the app.  Checkers and Baggers need to be given explicit privileges in the app to have access to the Checking (i.e. \"Finalize\") and Bagging sections.  Seeders/Sorters do not need access to the Archivers app. \n======= !-- - You need two S3 \"buckets\" (i.e., directories) for your harvested files.  The Harvesters will upload the files they harvest to the first bucket (\"pre-bag\" bucket)  In some cases, the Checkers will also upload improved versions of the files to the same pre-bag bucket  The Baggers will turn those files into bags and upload the bags to the second bucket (\"bag\" bucket)    A DataRefuge coordinator will set up the two S3 buckets for you  They will also make sure that the Uploader web applications used by Harvesters, Checkers, and Baggers \"knows\" about your event and has the name of your event listed as an option.  Most uploads to S3 will be done through the web-based Uploader application (http://drp-upload.herokuapp.com/ or http://drp-upload-bagger.herokuapp.com/). This application can handle files up to 5 Gigs.    One person at the event should be designated as the S3 System Administrator and will have direct access to the S3 buckets for the event. The S3 Sys Admin should be someone with advanced technical skills and will be responsible for 2 things:  Upload very large sets (beyond 5 Gigs) through an alternate method (provided by DataRefuge)  Keep the buckets cleaned up and organized.    Note that large files Uploaders need not be coders, but they should have a little experience working in command line, and computers with python 2.7.--", 
            "title": "Credentials"
        }, 
        {
            "location": "/advance-work/#credentials_1", 
            "text": "The Researchers/Harvesters/Checkers/Baggers need to have an account on the  Archivers app  You will need to generate invites for each one  within the app , and paste the URL generated in a slack Direct Message or an email  Each participant invited will automatically \"belongs\" to your event in the app    Checkers and Baggers need to be given explicit privileges in the app to have access to the Checking (i.e. \"Finalize\") and Bagging sections  Seeders/Sorters do not need access to the Archivers app        datarefuge/master:docs/advance-work.md", 
            "title": "Credentials"
        }, 
        {
            "location": "/advance-work/#other-supplies", 
            "text": "Make sure you have a few thumb drives to handle very large data sets (above 5 Gigs).", 
            "title": "Other supplies"
        }, 
        {
            "location": "/advance-work/#after-the-event", 
            "text": "Participants might want to continue the work started at the event remotely  This should be possible, as our workflow is meant to function in-person as well as remotely", 
            "title": "After the event"
        }, 
        {
            "location": "/seeding/", 
            "text": "What Do Seeders Do?\n\n\nSeeders canvass the resources of a given government agency, identifying important URLs and whether those URLs can be crawled by the Internet Archive's web crawler. They use the \nEDGI Nomination Chrome extension\n to nominate URLs to the \nEnd of Term (EOT) Web Archive\n if they are crawlable or to the Archivers app if they require manual archiving.\n\n\n\n  \nRecommended Skills\n \n  \n  Consider this path if you\u2019re comfortable browsing the web and have great attention to detail. An understanding of how web pages are structured will help you with this task.\n\n\n\n\nChoosing the Website\n\n\nSeeders use the \nEDGI Archiving Primers\n, or a similar set of resources, to identify important and at-risk data. Talk to the DataRescue organizers to learn more.\n\n\nCanvassing the Website and Evaluating Content\n\n\n\n\nStart exploring the website assigned, identifying important URLs.\n\n\nDecide whether the data on a page or website subsection can be automatically captured by the Internet Archive web crawler.\n\n\nEDGI's Guides\n have information critical to the seeding and sorting process:\n\n\nUnderstanding the Internet Archive Web Crawler\n\n\nSeeding the Internet Archive\u2019s Web Crawler\n\n\n\n\n\n\n\n\nCrawlable URLs\n\n\n\n\nURLs judged to be crawlable are \"nominated\" (equivalently, \"seeded\") to the End of Term project (EOT), using the \nEDGI Nomination Chrome extension\n.\n\n\n\n\nWherever possible, add in the Agency Office Code from the sub-primer.\n Talk to the DataRescue organizers to learn more.\n\n\nUncrawlable URLs\n\n\n\n\nIf URL is judged not crawlable, check one of the checkboxes next to the four types of uncrawlables in the Chrome Extension. This will add the URL to the Researching queue in the Archivers app.\n\n\nThe URL will be automatically associated with a universal unique identifier (UUID).\n\n\nYou can check whether the page or some files are archived using the Internet Archive's \nWayback Machine Chrome Extension\n\n\n\n\nNot Sure?\n\n\n\n\nThis sorting is only provisional; when in doubt, Seeders nominate the URL \nand\n mark it as possibly not crawlable.", 
            "title": "Seeding"
        }, 
        {
            "location": "/seeding/#what-do-seeders-do", 
            "text": "Seeders canvass the resources of a given government agency, identifying important URLs and whether those URLs can be crawled by the Internet Archive's web crawler. They use the  EDGI Nomination Chrome extension  to nominate URLs to the  End of Term (EOT) Web Archive  if they are crawlable or to the Archivers app if they require manual archiving.  \n   Recommended Skills     \n  Consider this path if you\u2019re comfortable browsing the web and have great attention to detail. An understanding of how web pages are structured will help you with this task.", 
            "title": "What Do Seeders Do?"
        }, 
        {
            "location": "/seeding/#choosing-the-website", 
            "text": "Seeders use the  EDGI Archiving Primers , or a similar set of resources, to identify important and at-risk data. Talk to the DataRescue organizers to learn more.", 
            "title": "Choosing the Website"
        }, 
        {
            "location": "/seeding/#canvassing-the-website-and-evaluating-content", 
            "text": "Start exploring the website assigned, identifying important URLs.  Decide whether the data on a page or website subsection can be automatically captured by the Internet Archive web crawler.  EDGI's Guides  have information critical to the seeding and sorting process:  Understanding the Internet Archive Web Crawler  Seeding the Internet Archive\u2019s Web Crawler", 
            "title": "Canvassing the Website and Evaluating Content"
        }, 
        {
            "location": "/seeding/#crawlable-urls", 
            "text": "URLs judged to be crawlable are \"nominated\" (equivalently, \"seeded\") to the End of Term project (EOT), using the  EDGI Nomination Chrome extension .   Wherever possible, add in the Agency Office Code from the sub-primer.  Talk to the DataRescue organizers to learn more.", 
            "title": "Crawlable URLs"
        }, 
        {
            "location": "/seeding/#uncrawlable-urls", 
            "text": "If URL is judged not crawlable, check one of the checkboxes next to the four types of uncrawlables in the Chrome Extension. This will add the URL to the Researching queue in the Archivers app.  The URL will be automatically associated with a universal unique identifier (UUID).  You can check whether the page or some files are archived using the Internet Archive's  Wayback Machine Chrome Extension", 
            "title": "Uncrawlable URLs"
        }, 
        {
            "location": "/seeding/#not-sure", 
            "text": "This sorting is only provisional; when in doubt, Seeders nominate the URL  and  mark it as possibly not crawlable.", 
            "title": "Not Sure?"
        }, 
        {
            "location": "/researching/", 
            "text": "What Do Researchers Do?\n\n\nResearchers review \"uncrawlables\" identified during \nSeeding\n, confirm the URL/dataset is indeed uncrawlable, and investigate how the dataset could be best harvested. Researchers need to have a good understanding of harvesting goals and have some familiarity with datasets.\n\n\n\n  \nRecommended Skills\n \n  \n  Consider this path if you have strong front-end web experience and enjoy research. An understanding of how federal data is organized (e.g. where \"master\" datasets are) would be valuable.\n\n\n\n\nGetting Set up as a Researcher\n\n\n\n\nEvent organizers (in-person or remote) will tell you how to volunteer for the Researcher role, either through Slack or a form.\n\n\nThey will send you an invite to the \nArchivers app\n, which helps us coordinate all the data archiving work we do.\n\n\nClick the invite link, and choose a username and a password. It is helpful to use the same username on the app and Slack.\n\n\n\n\n\n\nCreate an account on the DataRefuge Slack using this \nslack-in\n or use the Slack team recommended by your event organizers. This is where people share expertise and answer each other's questions.\n\n\nIf you need any assistance:\n\n\nTalk to your DataRescue guide if you are at an in-person event\n\n\nOr post questions on Slack in the \n#general\n channel (or other channel recommended by your event organizers).\n\n\n\n\n\n\n\n\n\n  \nUsing Archivers App\n \n  \n  Review our walkthrough video below and refer to the \nFAQ\n for any additional questions about the \nArchivers app\n. \n\n  \n\n  \n\n\n\n\n\nResearchers and Harvesters\n\n\n\n\nResearchers and Harvesters should work very closely together as their work will feed from each other and much communication is needed between the two roles.\n\n\nIt may be most effective for Researchers and Harvesters to work together in pairs or small groups. In some cases, a single person might be both a Researcher and a Harvester.\n\n\nAs a Researcher, make sure to check out the \nHarvesters documentation\n to familiarize yourself with their role.\n\n\n\n\nClaiming a Dataset to Research\n\n\n\n\nResearchers work on datasets that were listed as uncrawlable by Seeders.\n\n\nGo to the \nArchivers app\n, click \nURLS\n and then \nRESEARCH\n: all the URLs listed are ready to be researched.\n\n\nAvailable URLs are ones that have not been checked out by someone else, i.e. that do not have someone's name in the User column.\n\n\nPriority is indicated by the \u201c!\u201d field.  The range is from 0 to 10, with 10 being highest priority.\n\n\n\n\n\n\nSelect an available URL (you may decide to select a URL relevant to your area of expertise or assigned a high priority) and click its UUID to get to the detailed view, then click \nCheckout this URL\n. It is now ready for you to work on, and no one else can do anything to it while you have it checked out.\n\n\nWhile you go through the research process, make sure to report as much information as possible in the Archivers app, as this is the place were we collectively keep track of all the work done.\n\n\n\n\n\n  \nNote: URL vs UUID\n \n  \n  The \nURL\n is the link to examine and harvest, and the \nUUID\n is a canonical ID we use to connect the URL with the data in question. The UUID will have been generated earlier in the process. UUID stands for Universal Unique Identifier.\n\n\n\n\nEvaluating the Data\n\n\nGo to the URL and start inspecting the content.\n\n\nIs the data actually crawlable?\n\n\nAgain, see \nEDGI's Guides\n for a mostly non-technical introduction to the crawler:\n\n\n\n\nUnderstanding the Internet Archive Web Crawler\n\n\nSeeding the Internet Archive\u2019s Web Crawler\n\n\n\n\nSome additional technical notes for answering this:\n\n\n\n\nThere is no specific file size cutoff for what is crawlable, but large files should be manually captured anyway.\n\n\nFile types like ZIP, PDF, Excel, etc. are crawlable if they are linked, but it may be useful to archive them if they represent a meaningful dataset, or if there are many of them on a page.\n\n\nThe crawler can only follow HTTP links that appear directly in the DOM at load time. (That is, they should appear as \na href ...\n tags in the page source.)\nIf links are added by JavaScript or require submitting a form, they are not crawlable.\n\n\nThe crawler does not tolerate web frames (but it is straightforward to inspect a page to obtain the content in the frame directly, and then nominate \nthat\n).\n\n\nThe crawler recently added the ability to crawl FTP, but we will not rely on this; we will treat resources served over FTP as uncrawlable.\n\n\n\n\nYES:\n\n\nIf the URL is crawlable or you locate a crawlable URL that accesses the underlying dataset:\n\n\n\n\nNominate it using the \nEDGI Nomination Chrome Extension\n.\n\n\nClick the \nDo not harvest\n checkbox in the Research section in the Archivers app.\n\n\nClick \nCheckin this URL\n and move on to another URL.\n\n\n\n\nNO:\n\n\nIf it is confirmed not crawlable:\n\n\n\n\n\nSearch agency websites and data.gov for dataset entry points for your dataset collection.\n\n\nTips: Try to understand what datasets are underlying the web pages. Look for related entries in the Archivers app, and ensure that you aren't harvesting a subdirectory if you can harvest the entire directory. Often, data underlying dozens of pages or multiple \"access portal\" apps is also available as one structured data file.\n\n\nMake note of any better entry point in the \nRecommended Approach for Harvesting Data\n field, along with any other recommendations on how to proceed with this harvest.\n\n\n\n\n\n\nFill out all of the fields in the Research section to the best of your ability.\n\n\nOccasionally, URL's will have been nominated separately, but are actually different interfaces built on the same dataset. We want to scrape all of this data and do it exactly one time. The \nLink URL\n field lets you search for associated URLs; add any URLs that should be grouped into a single record.\n\n\n\n\nYES and NO:\n\n\nFor example, FTP address, mixed content, big data sets:\n\n\n\n\n\nNominate it anyway, but also follow the steps for uncrawlable content above.\n\n\nWhile we understand that this may result in some dataset duplication, this is not a concern. We are ensuring that the data is fully preserved and accessible.\n\n\n\n\nFinishing Up\n\n\n\n\nIn the Archivers app, make sure to fill out as much information as possible to document your work.\n\n\nCheck the Research checkbox (far right on the same line as the \"Research\" section heading) to mark that step as completed.\n\n\nClick \nSave\n.\n\n\nClick \nCheckin this URL\n, to release it and allow someone else to work on the next step.\n\n\nYou're done! Move on to the next URL!", 
            "title": "Researching"
        }, 
        {
            "location": "/researching/#what-do-researchers-do", 
            "text": "Researchers review \"uncrawlables\" identified during  Seeding , confirm the URL/dataset is indeed uncrawlable, and investigate how the dataset could be best harvested. Researchers need to have a good understanding of harvesting goals and have some familiarity with datasets.  \n   Recommended Skills     \n  Consider this path if you have strong front-end web experience and enjoy research. An understanding of how federal data is organized (e.g. where \"master\" datasets are) would be valuable.", 
            "title": "What Do Researchers Do?"
        }, 
        {
            "location": "/researching/#getting-set-up-as-a-researcher", 
            "text": "Event organizers (in-person or remote) will tell you how to volunteer for the Researcher role, either through Slack or a form.  They will send you an invite to the  Archivers app , which helps us coordinate all the data archiving work we do.  Click the invite link, and choose a username and a password. It is helpful to use the same username on the app and Slack.    Create an account on the DataRefuge Slack using this  slack-in  or use the Slack team recommended by your event organizers. This is where people share expertise and answer each other's questions.  If you need any assistance:  Talk to your DataRescue guide if you are at an in-person event  Or post questions on Slack in the  #general  channel (or other channel recommended by your event organizers).     \n   Using Archivers App     \n  Review our walkthrough video below and refer to the  FAQ  for any additional questions about the  Archivers app .", 
            "title": "Getting Set up as a Researcher"
        }, 
        {
            "location": "/researching/#researchers-and-harvesters", 
            "text": "Researchers and Harvesters should work very closely together as their work will feed from each other and much communication is needed between the two roles.  It may be most effective for Researchers and Harvesters to work together in pairs or small groups. In some cases, a single person might be both a Researcher and a Harvester.  As a Researcher, make sure to check out the  Harvesters documentation  to familiarize yourself with their role.", 
            "title": "Researchers and Harvesters"
        }, 
        {
            "location": "/researching/#claiming-a-dataset-to-research", 
            "text": "Researchers work on datasets that were listed as uncrawlable by Seeders.  Go to the  Archivers app , click  URLS  and then  RESEARCH : all the URLs listed are ready to be researched.  Available URLs are ones that have not been checked out by someone else, i.e. that do not have someone's name in the User column.  Priority is indicated by the \u201c!\u201d field.  The range is from 0 to 10, with 10 being highest priority.    Select an available URL (you may decide to select a URL relevant to your area of expertise or assigned a high priority) and click its UUID to get to the detailed view, then click  Checkout this URL . It is now ready for you to work on, and no one else can do anything to it while you have it checked out.  While you go through the research process, make sure to report as much information as possible in the Archivers app, as this is the place were we collectively keep track of all the work done.   \n   Note: URL vs UUID     \n  The  URL  is the link to examine and harvest, and the  UUID  is a canonical ID we use to connect the URL with the data in question. The UUID will have been generated earlier in the process. UUID stands for Universal Unique Identifier.", 
            "title": "Claiming a Dataset to Research"
        }, 
        {
            "location": "/researching/#evaluating-the-data", 
            "text": "Go to the URL and start inspecting the content.", 
            "title": "Evaluating the Data"
        }, 
        {
            "location": "/researching/#is-the-data-actually-crawlable", 
            "text": "Again, see  EDGI's Guides  for a mostly non-technical introduction to the crawler:   Understanding the Internet Archive Web Crawler  Seeding the Internet Archive\u2019s Web Crawler   Some additional technical notes for answering this:   There is no specific file size cutoff for what is crawlable, but large files should be manually captured anyway.  File types like ZIP, PDF, Excel, etc. are crawlable if they are linked, but it may be useful to archive them if they represent a meaningful dataset, or if there are many of them on a page.  The crawler can only follow HTTP links that appear directly in the DOM at load time. (That is, they should appear as  a href ...  tags in the page source.)\nIf links are added by JavaScript or require submitting a form, they are not crawlable.  The crawler does not tolerate web frames (but it is straightforward to inspect a page to obtain the content in the frame directly, and then nominate  that ).  The crawler recently added the ability to crawl FTP, but we will not rely on this; we will treat resources served over FTP as uncrawlable.   YES:  If the URL is crawlable or you locate a crawlable URL that accesses the underlying dataset:   Nominate it using the  EDGI Nomination Chrome Extension .  Click the  Do not harvest  checkbox in the Research section in the Archivers app.  Click  Checkin this URL  and move on to another URL.   NO:  If it is confirmed not crawlable:   Search agency websites and data.gov for dataset entry points for your dataset collection.  Tips: Try to understand what datasets are underlying the web pages. Look for related entries in the Archivers app, and ensure that you aren't harvesting a subdirectory if you can harvest the entire directory. Often, data underlying dozens of pages or multiple \"access portal\" apps is also available as one structured data file.  Make note of any better entry point in the  Recommended Approach for Harvesting Data  field, along with any other recommendations on how to proceed with this harvest.    Fill out all of the fields in the Research section to the best of your ability.  Occasionally, URL's will have been nominated separately, but are actually different interfaces built on the same dataset. We want to scrape all of this data and do it exactly one time. The  Link URL  field lets you search for associated URLs; add any URLs that should be grouped into a single record.   YES and NO:  For example, FTP address, mixed content, big data sets:   Nominate it anyway, but also follow the steps for uncrawlable content above.  While we understand that this may result in some dataset duplication, this is not a concern. We are ensuring that the data is fully preserved and accessible.", 
            "title": "Is the data actually crawlable?"
        }, 
        {
            "location": "/researching/#finishing-up", 
            "text": "In the Archivers app, make sure to fill out as much information as possible to document your work.  Check the Research checkbox (far right on the same line as the \"Research\" section heading) to mark that step as completed.  Click  Save .  Click  Checkin this URL , to release it and allow someone else to work on the next step.  You're done! Move on to the next URL!", 
            "title": "Finishing Up"
        }, 
        {
            "location": "/harvesting/", 
            "text": "What do Harvesters do?\n\n\nHarvesters take the \"uncrawlable\" data and try to figure out how to actually capture it based on the recommendations of the Researchers. This is a complex task which can require substantial technical expertise, and which requires different techniques for different tasks.\n\n\n\n  \nRecommended Skills\n \n  \n  Consider this path if you're a skilled technologist with a programming language of your choice (e.g., Python, JavaScript, C, etc.), are comfortable with the command line (bash, shell, powershell), or experience working with structured data. Experience in front-end web development a plus.\n\n\n\n\nImportant notes\n\n\n\n\n\n\nResearchers and Harvesters\n\n\n\n\nResearchers and Harvesters should work very closely together as their work will feed from each other and much communication is needed between the two roles\n\n\nFor instance they could work in pairs or in small groups\n\n\nIn some cases, a single person might be both a Researcher and a Harvester\n\n\n\n\n\n\nAs a Harvester, make sure to check out the \nResearchers documentation\n to familiarize yourself with their role\n\n\n\n\n\n\n\n\nThe notion of \"meaningful dataset\"\n\n\n\n\nYour role is to harvest datasets that are complete and \nmeaningful\n. By meaningful we mean: \"will the bag make sense to a scientist\"?\n\n\nFor instance, if a dataset is composed of a spreadsheet without any accompanying key or explanation of what the data represents, it might be completely impossible for a scientist to use it.\n\n\n\n\n\n\n\n\nGetting set up as a Harvester\n\n\n\n\nSkills recommended for this role: in general, Harvesters need to have some tech skills and a good understanding of harvesting goals.\n\n\nThe organizers of the event (in-person or remote) will tell you how to volunteer for the Harvester role, either through slack or a form.\n\n\nAs a result, they will send you an invite to the \nArchivers app\n, which helps us coordinate all the data archiving work we do.\n\n\nClick the invite link, and choose a user name and a password.\n\n\n\n\n\n\nMake sure you have an account on the DataRefuge slack (or other slack team recommended by your event organizers) This is where people share expertise and answer each other's questions.\n\n\nAsk your event organizer to send you an invite\n\n\nYou might also need to have some other software and utilities set up on your computer, depending on the harvested methods you will use.\n\n\nHarvesters should start by reading this document, which outlines the steps for constructing a proper data archive of the highest possible integrity. The primary focus of this document is on \nsemi-automated harvesting as part of a team\n, and the workflow described is best-suited for volunteers working to preserve small and medium-sized collections. Where possible, we try to link out to other options appropriate to other circumstances.\n\n\nIf you need any assistance:\n\n\nTalk to your DataRescue Guide if you are at an in-person event\n\n\nOr post questions on Slack in the #Researchers/Harvesters channel (or other channel recommended by your event organizers).\n\n\n\n\n\n\n\n\n\n\n\n\nHarvesting Toolkit\n\n\nFor in-depth information on tools and techniques to harvest open data, please check EDGI's extensive \ntoolkit\n.\n\n\n1. Claiming a dataset to harvest\n\n\n\n\nYou will work on datasets that were confirmed as unscrawlable by Researchers.\n\n\nGo to the \nArchivers app\n, click \nURLS\n and then \nHARVEST\n: all the URLs listed are ready to be harvested\n\n\nAvailable URLs are the ones that have not been checked out by someone else, that is, that do not have someone's name in the User column.\n\n\nSelect an available URL and click its UUID to get to the detailed view, then click \nCheck out this URL\n. It is now ready for you to work on, and no one else can do anything to it while you have it checked out.\n\n\nWhile you go through the harvesting process, make sure to report as much information as possible in the Archivers app, as this is the place were we collectively keep track of all the work done.\n\n\n\n\n\n  \nNote: URL vs UUID\n \n  \n  The \nURL\n is the link to examine and harvest, and the \nUUID\n is a canonical ID we use to connect the url with the data in question. The UUID will have been generated earlier earlier in the process. UUID stands for Universal Unique Identifier.\n\n\n\n\n2a. Classify Source Type \n archivability\n\n\nBefore doing anything, take a minute to understand what you're looking at. It's usually best to have a quick check of the url to confirm that this data in fact not crawlable. Often as part of the harvesting team, you'll be the first person with a higher level of technical knowledge to review the url in question.\n\n\nCheck for false-positives (content that is in fact crawlable)\n\n\nGenerally, any url that returns standard HTML, links to more \nHTML mimetype pages\n, and contains little-to-no non-html content, it's crawlable. \"View source\" from your browser of choice will help see what the crawler itself is seeing. If in fact the data can be crawled, make a note as such in the Google sheet, remove your name from the \"checkout\" column, notify the seeding / crawling team and they will make sure the link is crawled, and move on to another url.\n\n\nSome things to think about while reviewing a url\n\n\n\n\nDoes this page use javascript to render its content, especially to \ngenerate links\n or \ndynamically pull up images and pdf content\n? Crawlers generally cannot parse dynamically-generated content.\n\n\nDoes this url contain links to non-html content? (For example, zip files, pdfs, excel files, etc...)\n\n\nIs this url some sort of interface for a large database or service? (For example, an interactive map, api gateway, etc.)\n\n\nDoes this url contain instructions for connecting to a server, database, or other special source of data?\n\n\n\n\nCheck the terms of service!!!\n\n\nBefore you go any further, it is \nalways\n worth confirming that the data in question is in fact open for archiving. If the terms of service explicitly prohibit archiving, \nmake a note of it\n. Generally archive-a-thons are purposely only aimed at publically available data, but it is easy to follow a link away from a publically-available source onto a site that has different terms of service.\n\n\nData acquired outside terms of service is not usable\n\n\nIf there is harvestable data, the next step is to set up a directory (step three), and then choose the appropriate strategy for archiving!\n\n\n2b. Determine Scale of the Dataset\n\n\nIf the dataset you're looking at is quite large -- say, more than 1000 documents -- capturing it may require more elaborate programming than is described here, and it may be difficult to complete in the timeframe of the event. In that case, you may want to look outside the scope of this document and read the documentation of tools such as the \nEIS WARC archiver\n, which shows how to initiate a larger, fully automated harvest on a web-based virtual machine. Talk to your DataRescue Guide to determine how to best proceed.\n\n\n3. Generate HTML, JSON \n Directory\n\n\nTo get started click \nDownload Zip Starter\n, which will download an empty Zip archive structure for the data you are about to harvest.\nThe structure looks like this:\n\n\nDAFD2E80-965F-4989-8A77-843DE716D899\n    \u251c\u2500\u2500 DAFD2E80-965F-4989-8A77-843DE716D899.html\n    \u251c\u2500\u2500 DAFD2E80-965F-4989-8A77-843DE716D899.json\n    \u251c\u2500\u2500 /tools\n    \u2514\u2500\u2500 /data\n\n\n\n\nEach row in the above is:\n\n\nA directory named by the UUID\n    \u251c\u2500\u2500 a .html \nweb archive\n file of the url for future reference, named with the ID\n    \u251c\u2500\u2500 a .json metadata file that contains relevant metadata, named with the ID\n    \u251c\u2500\u2500 a /tools directory to include any scripts, notes \n files used to acquire the data\n    \u2514\u2500\u2500 a /data directory that contains the data in question\n\n\n\n\nUUID\n\n\nThe goal is to pass this finalized folder off for \n\"bagging\"\n. We repeatedly use the UUID so that we can programmatically work through this data later. It is important that the ID be copied \nexactly\n wherever it appears, with no leading or trailing spaces, and honoring case-sensitivity.\n\n\n[id].html file\n\n\nThe zip starter archive will automatically include a copy of the page corresponding to the URL. The html file gives the archive a snapshot of the page at the time of archiving which we can use to monitor for changing data in the future, and corroborate the provenance of the archive itself. We can also use the \n.html\n in conjunction with the scripts you'll include in the tools directory to replicate the archive in the future.\n\n\n\n\n\n[id].json file\n\n\nThe json file is one you'll create by hand to create a machine readable record of the archive. This file contains vital data, including the url that was archived, and date of archiving. The \nid.json readme\n goes into much more detail.\n\n\n4. Acquire the Data\n\n\nYour method for doing this will depend on the shape and size of the data you're dealing with. A few methods are described below.\n\n\n4a. Identify Data Links \n acquire them in a wget loop\n\n\nIf you encounter a page that links to lots of data (for example a \"downloads\" page), this approach may well work. It's important to only use this approach when you encounter \ndata\n, for example pdf's, .zip archives, .csv datasets, etc.\n\n\nThe tricky part of this approach is generating a list of urls to download from the page. If you're skilled with using scripts in combination with html-parsers (for example python's wonderful \nbeautiful-soup package\n), go for it. Otherwise, we've included the \njquery-url-extraction guide\n], which has the advantage of working within a browser and can operate on a page that has been modified by javascript.\n\n\nOur example dataset uses jquery-url, \nleveraging that tool to generate a list of urls to feed the wget loop\n.\n\n\n4b. Identify Data Links \n acquire them via WARCFactory\n\n\nFor search results from large document sets, you may need to do more sophisticated \"scraping\" and \"crawling\" -- again, check out tools built at previous events such as the \nEIS WARC archiver\n or the \nEPA Search Utils\n for ideas on how to proceed.\n\n\n4c. FTP download\n\n\nGovernment datasets are often stored on FTP. It's pretty easy to crawl these FTP sites with a simple Python script. Have a look at \ndownload_ftp_tree.py\n as an example. Note that the Internet Archive is doing an FTP crawl, so another option (especially if the dataset is large) would be to nominate this as a seed (though FTP seeds should be nominated \nseparately\n from http seeds).\n\n\n4d. API scrape / Custom Solution\n\n\nIf you encounter an API, chances are you'll have to build some sort of custom solution, or investigate a social angle. For example: asking someone with greater access for a database dump.\n\n\n4e. Automated Full Browser\n\n\nThe last resort of harvesting should be to drive it with a full web browser. It is slower than other approaches such as \nwget\n, \ncurl\n, or a headless browser. Additionally, this implementation is prone to issues where the resulting page is saved before it's done loading. There is a \nruby example\n.\n\n\n5. Write [id].json metadata, add /tools\n\n\nFrom there you'll want to fill out the metadata.json. Use the template below as a guide.\n\n\n\n\nThe json should match the information from the Harvester and use the following format:\n\n\n\n\n{\n \nIndividual source or seed URL\n: \nhttp://www.eia.gov/renewable/data.cfm\n,\n \nUUID\n : \nE30FA3CA-C5CB-41D5-8608-0650D1B6F105\n,\n \nid_agency\n : 2,\n \nid_subagency\n: ,\n \nid_org\n:,\n \nid_suborg\n:,\n \nInstitution facilitating the data capture creation and packaging\n: \nPenn Data Refuge\n,\n \nDate of capture\n: \n2017-01-17\n,\n \nFederal agency data acquired from\n: \nDepartment of Energy/U.S. Energy Information Administration\n,\n \nName of resource\n: \nRenewable and Alternative Fuels\n,\n \nFile formats contained in package\n: \n.pdf, .zip\n,\n \nType(s) of content in package\n: \ndatasets, codebooks\n,\n \nFree text description of capture process\n: \nMetadata was generated by viewing page and using spreadsheet descriptions where necessary, data was bulk downloaded from the page using wget -r on the seed URL and then bagged.\n,\n \nName of package creator\n: \nMallick Hossain and Ben Goldman\n\n }\n\n\n\n\n\n\nMake sure to save this as a .json file.\n\n\n\n\nIn addition, copy any scripts and tools you used into the /tools directory. It may seem strange to copy code multiple times, but this can help later to reconstruct the archiving process for further refinement later on.\n\n\nIt's worth using some judgement here. If a \"script\" you used includes an entire copy of JAVA, or some suite beyond a simple script, it may be better to document your process in a file and leave that in the tools directory instead.\n\n\nTips\n\n\n\n\nIf you encounter a Search bar, try entering \"*\" to check to see if that returns \"all results\".\n\n\nLeave the data unmodified\n\n  During the process you may feel inclined to clean things up, add structure to the data, etc. Avoid temptation. Your finished archive will be hashed so we can compare it later for changes, and it's important that we archive original, unmodified content.\n\n\n\n\n6. Uploading the data\n\n\n\n\nZip the all the files pertaining to your dataset within the zip started archive structure and confirm that it is named with the original UUID\n\n\nUpload the zip file by clicking \nUpload\n in the Archivers app, and selecting \nChoose File\n\n\nNote that files beyond 5 Gigs must be uploaded through the more advanced \nGenerate Upload Token\n option. This will require using the aws command line interface.\n\n\nPlease talk to your DataRescue guide/post on Slack in Baggers channel, if you are having issues with this more advanced method.\n\n\n\n\n\n\n\n\n7. Finishing up\n\n\n\n\nIn the Archivers app, make sure to fill out as much information as possible to document your work.\n\n\nCheck the Harvest checkbox (on the right-hand side) to mark that step as completed.\n\n\nClick \nSave\n.\n\n\nClick \nCheck in URL\n, to release it and allow someone else to work on the next step.\n\n\nYou're done! Move on to the next URL!", 
            "title": "Harvesting"
        }, 
        {
            "location": "/harvesting/#what-do-harvesters-do", 
            "text": "Harvesters take the \"uncrawlable\" data and try to figure out how to actually capture it based on the recommendations of the Researchers. This is a complex task which can require substantial technical expertise, and which requires different techniques for different tasks.  \n   Recommended Skills     \n  Consider this path if you're a skilled technologist with a programming language of your choice (e.g., Python, JavaScript, C, etc.), are comfortable with the command line (bash, shell, powershell), or experience working with structured data. Experience in front-end web development a plus.", 
            "title": "What do Harvesters do?"
        }, 
        {
            "location": "/harvesting/#important-notes", 
            "text": "Researchers and Harvesters   Researchers and Harvesters should work very closely together as their work will feed from each other and much communication is needed between the two roles  For instance they could work in pairs or in small groups  In some cases, a single person might be both a Researcher and a Harvester    As a Harvester, make sure to check out the  Researchers documentation  to familiarize yourself with their role     The notion of \"meaningful dataset\"   Your role is to harvest datasets that are complete and  meaningful . By meaningful we mean: \"will the bag make sense to a scientist\"?  For instance, if a dataset is composed of a spreadsheet without any accompanying key or explanation of what the data represents, it might be completely impossible for a scientist to use it.", 
            "title": "Important notes"
        }, 
        {
            "location": "/harvesting/#getting-set-up-as-a-harvester", 
            "text": "Skills recommended for this role: in general, Harvesters need to have some tech skills and a good understanding of harvesting goals.  The organizers of the event (in-person or remote) will tell you how to volunteer for the Harvester role, either through slack or a form.  As a result, they will send you an invite to the  Archivers app , which helps us coordinate all the data archiving work we do.  Click the invite link, and choose a user name and a password.    Make sure you have an account on the DataRefuge slack (or other slack team recommended by your event organizers) This is where people share expertise and answer each other's questions.  Ask your event organizer to send you an invite  You might also need to have some other software and utilities set up on your computer, depending on the harvested methods you will use.  Harvesters should start by reading this document, which outlines the steps for constructing a proper data archive of the highest possible integrity. The primary focus of this document is on  semi-automated harvesting as part of a team , and the workflow described is best-suited for volunteers working to preserve small and medium-sized collections. Where possible, we try to link out to other options appropriate to other circumstances.  If you need any assistance:  Talk to your DataRescue Guide if you are at an in-person event  Or post questions on Slack in the #Researchers/Harvesters channel (or other channel recommended by your event organizers).", 
            "title": "Getting set up as a Harvester"
        }, 
        {
            "location": "/harvesting/#harvesting-toolkit", 
            "text": "For in-depth information on tools and techniques to harvest open data, please check EDGI's extensive  toolkit .", 
            "title": "Harvesting Toolkit"
        }, 
        {
            "location": "/harvesting/#1-claiming-a-dataset-to-harvest", 
            "text": "You will work on datasets that were confirmed as unscrawlable by Researchers.  Go to the  Archivers app , click  URLS  and then  HARVEST : all the URLs listed are ready to be harvested  Available URLs are the ones that have not been checked out by someone else, that is, that do not have someone's name in the User column.  Select an available URL and click its UUID to get to the detailed view, then click  Check out this URL . It is now ready for you to work on, and no one else can do anything to it while you have it checked out.  While you go through the harvesting process, make sure to report as much information as possible in the Archivers app, as this is the place were we collectively keep track of all the work done.   \n   Note: URL vs UUID     \n  The  URL  is the link to examine and harvest, and the  UUID  is a canonical ID we use to connect the url with the data in question. The UUID will have been generated earlier earlier in the process. UUID stands for Universal Unique Identifier.", 
            "title": "1. Claiming a dataset to harvest"
        }, 
        {
            "location": "/harvesting/#2a-classify-source-type-archivability", 
            "text": "Before doing anything, take a minute to understand what you're looking at. It's usually best to have a quick check of the url to confirm that this data in fact not crawlable. Often as part of the harvesting team, you'll be the first person with a higher level of technical knowledge to review the url in question.", 
            "title": "2a. Classify Source Type &amp; archivability"
        }, 
        {
            "location": "/harvesting/#check-for-false-positives-content-that-is-in-fact-crawlable", 
            "text": "Generally, any url that returns standard HTML, links to more  HTML mimetype pages , and contains little-to-no non-html content, it's crawlable. \"View source\" from your browser of choice will help see what the crawler itself is seeing. If in fact the data can be crawled, make a note as such in the Google sheet, remove your name from the \"checkout\" column, notify the seeding / crawling team and they will make sure the link is crawled, and move on to another url.", 
            "title": "Check for false-positives (content that is in fact crawlable)"
        }, 
        {
            "location": "/harvesting/#some-things-to-think-about-while-reviewing-a-url", 
            "text": "Does this page use javascript to render its content, especially to  generate links  or  dynamically pull up images and pdf content ? Crawlers generally cannot parse dynamically-generated content.  Does this url contain links to non-html content? (For example, zip files, pdfs, excel files, etc...)  Is this url some sort of interface for a large database or service? (For example, an interactive map, api gateway, etc.)  Does this url contain instructions for connecting to a server, database, or other special source of data?", 
            "title": "Some things to think about while reviewing a url"
        }, 
        {
            "location": "/harvesting/#check-the-terms-of-service", 
            "text": "Before you go any further, it is  always  worth confirming that the data in question is in fact open for archiving. If the terms of service explicitly prohibit archiving,  make a note of it . Generally archive-a-thons are purposely only aimed at publically available data, but it is easy to follow a link away from a publically-available source onto a site that has different terms of service.  Data acquired outside terms of service is not usable  If there is harvestable data, the next step is to set up a directory (step three), and then choose the appropriate strategy for archiving!", 
            "title": "Check the terms of service!!!"
        }, 
        {
            "location": "/harvesting/#2b-determine-scale-of-the-dataset", 
            "text": "If the dataset you're looking at is quite large -- say, more than 1000 documents -- capturing it may require more elaborate programming than is described here, and it may be difficult to complete in the timeframe of the event. In that case, you may want to look outside the scope of this document and read the documentation of tools such as the  EIS WARC archiver , which shows how to initiate a larger, fully automated harvest on a web-based virtual machine. Talk to your DataRescue Guide to determine how to best proceed.", 
            "title": "2b. Determine Scale of the Dataset"
        }, 
        {
            "location": "/harvesting/#3-generate-html-json-directory", 
            "text": "To get started click  Download Zip Starter , which will download an empty Zip archive structure for the data you are about to harvest.\nThe structure looks like this:  DAFD2E80-965F-4989-8A77-843DE716D899\n    \u251c\u2500\u2500 DAFD2E80-965F-4989-8A77-843DE716D899.html\n    \u251c\u2500\u2500 DAFD2E80-965F-4989-8A77-843DE716D899.json\n    \u251c\u2500\u2500 /tools\n    \u2514\u2500\u2500 /data  Each row in the above is:  A directory named by the UUID\n    \u251c\u2500\u2500 a .html  web archive  file of the url for future reference, named with the ID\n    \u251c\u2500\u2500 a .json metadata file that contains relevant metadata, named with the ID\n    \u251c\u2500\u2500 a /tools directory to include any scripts, notes   files used to acquire the data\n    \u2514\u2500\u2500 a /data directory that contains the data in question", 
            "title": "3. Generate HTML, JSON &amp; Directory"
        }, 
        {
            "location": "/harvesting/#uuid", 
            "text": "The goal is to pass this finalized folder off for  \"bagging\" . We repeatedly use the UUID so that we can programmatically work through this data later. It is important that the ID be copied  exactly  wherever it appears, with no leading or trailing spaces, and honoring case-sensitivity.", 
            "title": "UUID"
        }, 
        {
            "location": "/harvesting/#idhtml-file", 
            "text": "The zip starter archive will automatically include a copy of the page corresponding to the URL. The html file gives the archive a snapshot of the page at the time of archiving which we can use to monitor for changing data in the future, and corroborate the provenance of the archive itself. We can also use the  .html  in conjunction with the scripts you'll include in the tools directory to replicate the archive in the future.", 
            "title": "[id].html file"
        }, 
        {
            "location": "/harvesting/#idjson-file", 
            "text": "The json file is one you'll create by hand to create a machine readable record of the archive. This file contains vital data, including the url that was archived, and date of archiving. The  id.json readme  goes into much more detail.", 
            "title": "[id].json file"
        }, 
        {
            "location": "/harvesting/#4-acquire-the-data", 
            "text": "Your method for doing this will depend on the shape and size of the data you're dealing with. A few methods are described below.", 
            "title": "4. Acquire the Data"
        }, 
        {
            "location": "/harvesting/#4a-identify-data-links-acquire-them-in-a-wget-loop", 
            "text": "If you encounter a page that links to lots of data (for example a \"downloads\" page), this approach may well work. It's important to only use this approach when you encounter  data , for example pdf's, .zip archives, .csv datasets, etc.  The tricky part of this approach is generating a list of urls to download from the page. If you're skilled with using scripts in combination with html-parsers (for example python's wonderful  beautiful-soup package ), go for it. Otherwise, we've included the  jquery-url-extraction guide ], which has the advantage of working within a browser and can operate on a page that has been modified by javascript.  Our example dataset uses jquery-url,  leveraging that tool to generate a list of urls to feed the wget loop .", 
            "title": "4a. Identify Data Links &amp; acquire them in a wget loop"
        }, 
        {
            "location": "/harvesting/#4b-identify-data-links-acquire-them-via-warcfactory", 
            "text": "For search results from large document sets, you may need to do more sophisticated \"scraping\" and \"crawling\" -- again, check out tools built at previous events such as the  EIS WARC archiver  or the  EPA Search Utils  for ideas on how to proceed.", 
            "title": "4b. Identify Data Links &amp; acquire them via WARCFactory"
        }, 
        {
            "location": "/harvesting/#4c-ftp-download", 
            "text": "Government datasets are often stored on FTP. It's pretty easy to crawl these FTP sites with a simple Python script. Have a look at  download_ftp_tree.py  as an example. Note that the Internet Archive is doing an FTP crawl, so another option (especially if the dataset is large) would be to nominate this as a seed (though FTP seeds should be nominated  separately  from http seeds).", 
            "title": "4c. FTP download"
        }, 
        {
            "location": "/harvesting/#4d-api-scrape-custom-solution", 
            "text": "If you encounter an API, chances are you'll have to build some sort of custom solution, or investigate a social angle. For example: asking someone with greater access for a database dump.", 
            "title": "4d. API scrape / Custom Solution"
        }, 
        {
            "location": "/harvesting/#4e-automated-full-browser", 
            "text": "The last resort of harvesting should be to drive it with a full web browser. It is slower than other approaches such as  wget ,  curl , or a headless browser. Additionally, this implementation is prone to issues where the resulting page is saved before it's done loading. There is a  ruby example .", 
            "title": "4e. Automated Full Browser"
        }, 
        {
            "location": "/harvesting/#5-write-idjson-metadata-add-tools", 
            "text": "From there you'll want to fill out the metadata.json. Use the template below as a guide.   The json should match the information from the Harvester and use the following format:   {\n  Individual source or seed URL :  http://www.eia.gov/renewable/data.cfm ,\n  UUID  :  E30FA3CA-C5CB-41D5-8608-0650D1B6F105 ,\n  id_agency  : 2,\n  id_subagency : ,\n  id_org :,\n  id_suborg :,\n  Institution facilitating the data capture creation and packaging :  Penn Data Refuge ,\n  Date of capture :  2017-01-17 ,\n  Federal agency data acquired from :  Department of Energy/U.S. Energy Information Administration ,\n  Name of resource :  Renewable and Alternative Fuels ,\n  File formats contained in package :  .pdf, .zip ,\n  Type(s) of content in package :  datasets, codebooks ,\n  Free text description of capture process :  Metadata was generated by viewing page and using spreadsheet descriptions where necessary, data was bulk downloaded from the page using wget -r on the seed URL and then bagged. ,\n  Name of package creator :  Mallick Hossain and Ben Goldman \n }   Make sure to save this as a .json file.   In addition, copy any scripts and tools you used into the /tools directory. It may seem strange to copy code multiple times, but this can help later to reconstruct the archiving process for further refinement later on.  It's worth using some judgement here. If a \"script\" you used includes an entire copy of JAVA, or some suite beyond a simple script, it may be better to document your process in a file and leave that in the tools directory instead.", 
            "title": "5. Write [id].json metadata, add /tools"
        }, 
        {
            "location": "/harvesting/#tips", 
            "text": "If you encounter a Search bar, try entering \"*\" to check to see if that returns \"all results\".  Leave the data unmodified \n  During the process you may feel inclined to clean things up, add structure to the data, etc. Avoid temptation. Your finished archive will be hashed so we can compare it later for changes, and it's important that we archive original, unmodified content.", 
            "title": "Tips"
        }, 
        {
            "location": "/harvesting/#6-uploading-the-data", 
            "text": "Zip the all the files pertaining to your dataset within the zip started archive structure and confirm that it is named with the original UUID  Upload the zip file by clicking  Upload  in the Archivers app, and selecting  Choose File  Note that files beyond 5 Gigs must be uploaded through the more advanced  Generate Upload Token  option. This will require using the aws command line interface.  Please talk to your DataRescue guide/post on Slack in Baggers channel, if you are having issues with this more advanced method.", 
            "title": "6. Uploading the data"
        }, 
        {
            "location": "/harvesting/#7-finishing-up", 
            "text": "In the Archivers app, make sure to fill out as much information as possible to document your work.  Check the Harvest checkbox (on the right-hand side) to mark that step as completed.  Click  Save .  Click  Check in URL , to release it and allow someone else to work on the next step.  You're done! Move on to the next URL!", 
            "title": "7. Finishing up"
        }, 
        {
            "location": "/bagging/", 
            "text": "HEAD\n\n\nWhat do Baggers do?\n\n\nBaggers do some quality assurance on the dataset to make sure the content is correct and corresponds to what was described in the spreadsheet. Then they package the data into a bagit file (or \"bag\"), which includes basic technical metadata and upload it to final DataRefuge destination.\n\n\nWhat Do Baggers Do?\n\n\nBaggers do some quality assurance on the dataset to make sure the content is correct and corresponds to what was described in the spreadsheet. Then they package the data into a bagit file (or \"bag\"), which includes basic technical metadata, and upload it to the final DataRefuge destination.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndatarefuge/master\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \nRecommended Skills\n \n  \n  Consider this path if you have data or web archiving experience, or have strong tech skills and an attention to detail.\n\n\n\n\n HEAD\n\n\nGetting set up as a Bagger\n\n\n\n\nSkills recommended for this role: in general, Baggers need to have some tech skills and a good understanding of harvesting goals.\n\n\nApply to become a Bagger:\n\n\nBy filling out \nthis form\n\n\nNote that an email address is required to apply.\n\n\nNote also that you should be willing to have your real name be associated with the datasets, to follow archival best practices (see \nguidelines on archival best practices for Data Refuge\n for more information).\n\n\n\n\n\n\n\n\n\n\nThe organizers of the event (in-person or remote) will send you an invite to the \nArchivers app\n, which helps us coordinate all the data archiving work we do.\n\n\nClick the invite link, and choose a user name and a password.\n\n\n\n\n\n\nMake sure you have an account on the DataRefuge slack (or other slack team recommended by your event organizers) This is where people share expertise and answer each other's questions.\n\n\nAsk your event organizer to send you an invite\n\n\nGet set up with Python and the Python script to make a bag at the command line\n\n\nhttps://github.com/LibraryOfCongress/bagit-python\n\n\nIf you need any assistance:\n\n\nTalk to your DataRescue Guide if you are at an in-person event\n\n\nOr post questions on Slack in the #Baggers channel(or other channel recommended by your event organizers).\n\n\n\n\nClaiming a dataset for bagging\n\n\n\n\nYou will work on datasets that were harvested by Harvesters.\n\n\nGo to the \nArchivers app\n, click \nURLS\n and then \nBAG\n: all the URLs listed are ready to be bagged\n\n\nAvailable URLs are the ones that have not been checked out by someone else, that is, that do not have someone's name in the User column.\n\n\n\n\n\n\nSelect an available URL and click its UUID to get to the detailed view, then click \nCheck out this URL\n. It is now ready for you to work on, and no one else can do anything to it while you have it checked out.\n\n\nWhile you go through the bagging process, make sure to report as much information as possible in the Archivers app, as this is the place were we collectively keep track of all the work done.\n\n\n\n\n\n  \nNote: URL vs UUID\n \n  \n  The \nURL\n is the link to examine and harvest, and the \nUUID\n is a canonical ID we use to connect the url with the data in question. The UUID will have been generated earlier earlier in the process. UUID stands for Universal Unique Identifier.\n=======\n**Note that the Checker role is currently performed by Baggers (See Quality Assurance step below). Eventually we would like the two to be separate, but the Archivers app does not offer that capability at this time.**\n\n## Getting Set up as a Bagger\n\n- Apply to become a Bagger by filling out [this form](https://docs.google.com/a/temple.edu/forms/d/e/1FAIpQLSfh9YIFnDrc-Cuc0hTd-U37J3D8xw8K7VXmzWkPs6Y5Q0wfVg/viewform)\n    - Note that an email address is required to apply.\n    - Note also that you should be willing to have your real name be associated with the datasets, to follow archival best practices (see [guidelines on archival best practices for Data Refuge](http://www.ppehlab.org/blogposts/2017/2/1/data-refuge-rests-on-a-clear-chain-of-custody) for more information).\n- The organizers of the event (in-person or remote) will send you an invite to the [Archivers app](http://www.archivers.space/), which helps us coordinate all the data archiving work we do.\n    - Click the invite link, and choose a user name and a password.\n- Create an account on the DataRefuge Slack using this [slack-in](https://rauchg-slackin-qonsfhhvxs.now.sh/) (or use the Slack team recommended by your event organizers). This is where people share expertise and answer each other's questions.   \n- Get set up with Python and the Python script to make a bag at the command line: [https://github.com/LibraryOfCongress/bagit-python](https://github.com/LibraryOfCongress/bagit-python)\n- If you need any assistance:\n     - Talk to your DataRescue guide if you are at an in-person event.\n     - Or post questions on Slack in the #Baggers channel (or other channel recommended by your event organizers).\n\n## Claiming a Dataset for Bagging\n\n- You will work on datasets that were harvested by Harvesters.\n- Go to the [Archivers app](http://www.archivers.space/), click `URLS` and then `BAG`: all the URLs listed are ready to be bagged.\n    - Available URLs are the ones that have not been checked out by someone else, i.e. that do not have someone's name in the User column.\n- Select an available URL and click its UUID to get to the detailed view, then click `Checkout this URL`. It is now ready for you to work on, and no one else can do anything to it while you have it checked out.\n- While you go through the bagging process, make sure to report as much information as possible in the Archivers app, as this is the place where we collectively keep track of all the work done.\n\n\n\n  \nNote: URL vs UUID\n \n  \n  The \nURL\n is the link to examine and harvest, and the \nUUID\n is a canonical ID we use to connect the URL with the data in question. The UUID will have been generated earlier in the process. UUID stands for Universal Unique Identifier.\n>>>>>>> datarefuge/master\n\n\n\n## Downloading \n&\n Opening the Dataset\n\n\n<\n<\n<\n<\n<\n<\n<\n HEAD\n- The zipped dataset that is ready to be bagged is under `Harvest Url / Location` in the the Archivers app. Download it to your laptop, and unzip it.\n- Extra check: Is this URL truly ready to bag?\n    - While everybody is doing their best to provide accurate information, occasionally a URL will be presented as \"ready to bag\", but, in fact, is not. Symptoms include:\n    - There is no value in the \"Harvest Url / Location\" field\n    - There is a note in the Harvest section that seem to indicate that the harvest was only partially performed.  \n        - In either case, uncheck the \"Harvest\" checkbox, and add a note in the Harvest note, indicating that the URL does not seem ready for bagging and needs to be reviewed by a Harvester.\n=======\n- The zipped dataset that is ready to be bagged is under `Harvest Url / Location` in the the Archivers app. Download it to your laptop and unzip it.\n- Extra check: Is this URL truly ready to bag?\n    - While everybody is doing their best to provide accurate information, occasionally a URL will be presented as \"ready to bag\" but, in fact, is not. Symptoms include:\n        - There is no value in the \"Harvest Url / Location\" field.\n        - There is a note in the Harvest section that seems to indicate that the harvest was only partially performed.  \n        - In either case, uncheck the \"Harvest\" checkbox, and add a note in the `Notes From Harvest` field indicating that the URL does not seem ready for bagging and needs to be reviewed by a Harvester.\n>>>>>>> datarefuge/master\n\n## Quality Assurance\n\n- Confirm the harvested files:\n    - Go to the original URL and check that the dataset is complete and accurate.\n    - You also need to check that the dataset is meaningful, that is: \"will the bag make sense to a scientist\"?\n    For instance, if a dataset is composed of a spreadsheet without any accompanying key or explanation of what the data represents, it might be completely impossible for a scientist to use it.\n\n<\n<\n<\n<\n<\n<\n<\n HEAD\n    - Spot check to make sure the files open properly and are not faulty in any way.\n- Confirm contents of Json file:\n    - The json should match the information from the Harvester and use the following format:  \n\n\u0002wzxhzdk:0\u0003\n\n{\n    \"Date of capture\": \"Fri Feb 24 2017 21:44:07 GMT-0800 (PST)\",\n    \"File formats contained in package\": \".xls, .zip\",\n    \"Free text description of capture process\": \"Metadata was generated by viewing page, data was bulk downloaded using download_ftp_tree.py and then bagged.\",\n    \"Individual source or seed URL\": \"ftp://podaac-ftp.jpl.nasa.gov/allData/nimbus7\",\n    \"Institution facilitating the data capture creation and packaging\": \"DataRescue SF Bay\",\n    \"Name of package creator\": \"JohnDoe\",\n    \"Name of resource\": \"Nimbus-7 SMMR global 60km gridded ocean parameters for 1979 - 1984\",\n    \"Type(s) of content in package\": \"These files are the Wentz Nimbus-7 SMMR global 60km gridded ocean parameters for 1979 - 1984.  There are 72 files which breaks down to 1 file per month. So, NIMBUS7-SMMR00000.dat is January 1979, NIMBUS7-SMMR00001.dat is February 1979, etc.\",\n    \"UUID\": \"F3499E3B-7517-4C06-A661-72B4DA13A2A2\",\n    \"recommended_approach\": \"\",\n    \"significance\": \"These files are the Wentz Nimbus-7 SMMR global 60km gridded ocean parameters for 1979 - 1984.  There are 72 files which breaks down to 1 file per month. So, NIMBUS7-SMMR00000.dat is January 1979, NIMBUS7-SMMR00001.dat is February 1979, etc.\",\n    \"title\": \"Nimbus-7 SMMR global 60km gridded ocean parameters for 1979 - 1984\",\n    \"url\": \"ftp://podaac-ftp.jpl.nasa.gov/allData/nimbus7\"\n>>>>>>> datarefuge/master\n}\n\n\u0002wzxhzdk:1\u0003\n\nbagit.py --contact-name '[your name]' /directory/to/bag\n\n\u0002wzxhzdk:2\u0003\n\nbagit.py --contact-name '[your name]' /directory/to/bag\n\n\u0002wzxhzdk:3\u0003\n\nbagit.py --validate [directory/of/bag/to/validate]\n\n\u0002wzxhzdk:4\u0003\n\nbagit.py --validate [directory/of/bag/to/validate]\n```\n\n\n<\n<\n<\n<\n<\n<\n<\n HEAD\n- If it comes back as valid, open the bag and spot-check to make sure everything looks the same as when you uploaded it. (This will not affect the validity of the bags already uploaded). If all seems right, proceed to the rest of the quality assurance steps. If it does not come back as valid, make a note of the error, review your steps, and re-bag the file. If you continue to get invalid bags, please see a DataRescue guide or reach out to the #bagging Slack channel.\n- Fill out as much information as possible in the `Notes From Bagging` field in the Archivers app to document your work.\n- Check the checkbox that certifies this is a \"well-checked bag\"\n- Check the Bag checkbox (on the right-hand side) to mark that step as completed.\n- Click `Save`.\n- Click `Check in URL` to release it and allow someone else to work on the next step.\n=======\n- If it comes back as valid, open the bag and spot-check to make sure everything looks the same as when you uploaded it (this will not affect the validity of the bags already uploaded). If all seems right, proceed to the rest of the quality assurance steps. If it does not come back as valid, make a note of the error, review your steps, and re-bag the file. If you continue to get invalid bags, please see a DataRescue guide or reach out in the Baggers Slack channel.\n- Fill out as much information as possible in the `Notes From Bagging` field in the Archivers app to document your work.\n- Check the checkbox that certifies this is a \"well-checked bag\".\n- Check the Bag checkbox (far right on the same line as the \"Bag\" section heading) to mark that step as completed.\n- Click `Save`.\n- Click `Checkin this URL` to release it and allow someone else to work on the next step.\n>>>>>>> datarefuge/master", 
            "title": "Checking/Bagging"
        }, 
        {
            "location": "/bagging/#what-do-baggers-do", 
            "text": "", 
            "title": "What do Baggers do?"
        }, 
        {
            "location": "/bagging/#baggers-do-some-quality-assurance-on-the-dataset-to-make-sure-the-content-is-correct-and-corresponds-to-what-was-described-in-the-spreadsheet-then-they-package-the-data-into-a-bagit-file-or-bag-which-includes-basic-technical-metadata-and-upload-it-to-final-datarefuge-destination", 
            "text": "", 
            "title": "Baggers do some quality assurance on the dataset to make sure the content is correct and corresponds to what was described in the spreadsheet. Then they package the data into a bagit file (or \"bag\"), which includes basic technical metadata and upload it to final DataRefuge destination."
        }, 
        {
            "location": "/bagging/#what-do-baggers-do_1", 
            "text": "Baggers do some quality assurance on the dataset to make sure the content is correct and corresponds to what was described in the spreadsheet. Then they package the data into a bagit file (or \"bag\"), which includes basic technical metadata, and upload it to the final DataRefuge destination.         datarefuge/master         \n   Recommended Skills     \n  Consider this path if you have data or web archiving experience, or have strong tech skills and an attention to detail.   HEAD", 
            "title": "What Do Baggers Do?"
        }, 
        {
            "location": "/bagging/#getting-set-up-as-a-bagger", 
            "text": "Skills recommended for this role: in general, Baggers need to have some tech skills and a good understanding of harvesting goals.  Apply to become a Bagger:  By filling out  this form  Note that an email address is required to apply.  Note also that you should be willing to have your real name be associated with the datasets, to follow archival best practices (see  guidelines on archival best practices for Data Refuge  for more information).      The organizers of the event (in-person or remote) will send you an invite to the  Archivers app , which helps us coordinate all the data archiving work we do.  Click the invite link, and choose a user name and a password.    Make sure you have an account on the DataRefuge slack (or other slack team recommended by your event organizers) This is where people share expertise and answer each other's questions.  Ask your event organizer to send you an invite  Get set up with Python and the Python script to make a bag at the command line  https://github.com/LibraryOfCongress/bagit-python  If you need any assistance:  Talk to your DataRescue Guide if you are at an in-person event  Or post questions on Slack in the #Baggers channel(or other channel recommended by your event organizers).", 
            "title": "Getting set up as a Bagger"
        }, 
        {
            "location": "/bagging/#claiming-a-dataset-for-bagging", 
            "text": "You will work on datasets that were harvested by Harvesters.  Go to the  Archivers app , click  URLS  and then  BAG : all the URLs listed are ready to be bagged  Available URLs are the ones that have not been checked out by someone else, that is, that do not have someone's name in the User column.    Select an available URL and click its UUID to get to the detailed view, then click  Check out this URL . It is now ready for you to work on, and no one else can do anything to it while you have it checked out.  While you go through the bagging process, make sure to report as much information as possible in the Archivers app, as this is the place were we collectively keep track of all the work done.   \n   Note: URL vs UUID     \n  The  URL  is the link to examine and harvest, and the  UUID  is a canonical ID we use to connect the url with the data in question. The UUID will have been generated earlier earlier in the process. UUID stands for Universal Unique Identifier.\n=======\n**Note that the Checker role is currently performed by Baggers (See Quality Assurance step below). Eventually we would like the two to be separate, but the Archivers app does not offer that capability at this time.**\n\n## Getting Set up as a Bagger\n\n- Apply to become a Bagger by filling out [this form](https://docs.google.com/a/temple.edu/forms/d/e/1FAIpQLSfh9YIFnDrc-Cuc0hTd-U37J3D8xw8K7VXmzWkPs6Y5Q0wfVg/viewform)\n    - Note that an email address is required to apply.\n    - Note also that you should be willing to have your real name be associated with the datasets, to follow archival best practices (see [guidelines on archival best practices for Data Refuge](http://www.ppehlab.org/blogposts/2017/2/1/data-refuge-rests-on-a-clear-chain-of-custody) for more information).\n- The organizers of the event (in-person or remote) will send you an invite to the [Archivers app](http://www.archivers.space/), which helps us coordinate all the data archiving work we do.\n    - Click the invite link, and choose a user name and a password.\n- Create an account on the DataRefuge Slack using this [slack-in](https://rauchg-slackin-qonsfhhvxs.now.sh/) (or use the Slack team recommended by your event organizers). This is where people share expertise and answer each other's questions.   \n- Get set up with Python and the Python script to make a bag at the command line: [https://github.com/LibraryOfCongress/bagit-python](https://github.com/LibraryOfCongress/bagit-python)\n- If you need any assistance:\n     - Talk to your DataRescue guide if you are at an in-person event.\n     - Or post questions on Slack in the #Baggers channel (or other channel recommended by your event organizers).\n\n## Claiming a Dataset for Bagging\n\n- You will work on datasets that were harvested by Harvesters.\n- Go to the [Archivers app](http://www.archivers.space/), click `URLS` and then `BAG`: all the URLs listed are ready to be bagged.\n    - Available URLs are the ones that have not been checked out by someone else, i.e. that do not have someone's name in the User column.\n- Select an available URL and click its UUID to get to the detailed view, then click `Checkout this URL`. It is now ready for you to work on, and no one else can do anything to it while you have it checked out.\n- While you go through the bagging process, make sure to report as much information as possible in the Archivers app, as this is the place where we collectively keep track of all the work done. \n   Note: URL vs UUID     \n  The  URL  is the link to examine and harvest, and the  UUID  is a canonical ID we use to connect the URL with the data in question. The UUID will have been generated earlier in the process. UUID stands for Universal Unique Identifier.\n>>>>>>> datarefuge/master \n\n## Downloading  &  Opening the Dataset < < < < < < <  HEAD\n- The zipped dataset that is ready to be bagged is under `Harvest Url / Location` in the the Archivers app. Download it to your laptop, and unzip it.\n- Extra check: Is this URL truly ready to bag?\n    - While everybody is doing their best to provide accurate information, occasionally a URL will be presented as \"ready to bag\", but, in fact, is not. Symptoms include:\n    - There is no value in the \"Harvest Url / Location\" field\n    - There is a note in the Harvest section that seem to indicate that the harvest was only partially performed.  \n        - In either case, uncheck the \"Harvest\" checkbox, and add a note in the Harvest note, indicating that the URL does not seem ready for bagging and needs to be reviewed by a Harvester.\n=======\n- The zipped dataset that is ready to be bagged is under `Harvest Url / Location` in the the Archivers app. Download it to your laptop and unzip it.\n- Extra check: Is this URL truly ready to bag?\n    - While everybody is doing their best to provide accurate information, occasionally a URL will be presented as \"ready to bag\" but, in fact, is not. Symptoms include:\n        - There is no value in the \"Harvest Url / Location\" field.\n        - There is a note in the Harvest section that seems to indicate that the harvest was only partially performed.  \n        - In either case, uncheck the \"Harvest\" checkbox, and add a note in the `Notes From Harvest` field indicating that the URL does not seem ready for bagging and needs to be reviewed by a Harvester.\n>>>>>>> datarefuge/master\n\n## Quality Assurance\n\n- Confirm the harvested files:\n    - Go to the original URL and check that the dataset is complete and accurate.\n    - You also need to check that the dataset is meaningful, that is: \"will the bag make sense to a scientist\"?\n    For instance, if a dataset is composed of a spreadsheet without any accompanying key or explanation of what the data represents, it might be completely impossible for a scientist to use it. < < < < < < <  HEAD\n    - Spot check to make sure the files open properly and are not faulty in any way.\n- Confirm contents of Json file:\n    - The json should match the information from the Harvester and use the following format:  \n\n\u0002wzxhzdk:0\u0003\n\n{\n    \"Date of capture\": \"Fri Feb 24 2017 21:44:07 GMT-0800 (PST)\",\n    \"File formats contained in package\": \".xls, .zip\",\n    \"Free text description of capture process\": \"Metadata was generated by viewing page, data was bulk downloaded using download_ftp_tree.py and then bagged.\",\n    \"Individual source or seed URL\": \"ftp://podaac-ftp.jpl.nasa.gov/allData/nimbus7\",\n    \"Institution facilitating the data capture creation and packaging\": \"DataRescue SF Bay\",\n    \"Name of package creator\": \"JohnDoe\",\n    \"Name of resource\": \"Nimbus-7 SMMR global 60km gridded ocean parameters for 1979 - 1984\",\n    \"Type(s) of content in package\": \"These files are the Wentz Nimbus-7 SMMR global 60km gridded ocean parameters for 1979 - 1984.  There are 72 files which breaks down to 1 file per month. So, NIMBUS7-SMMR00000.dat is January 1979, NIMBUS7-SMMR00001.dat is February 1979, etc.\",\n    \"UUID\": \"F3499E3B-7517-4C06-A661-72B4DA13A2A2\",\n    \"recommended_approach\": \"\",\n    \"significance\": \"These files are the Wentz Nimbus-7 SMMR global 60km gridded ocean parameters for 1979 - 1984.  There are 72 files which breaks down to 1 file per month. So, NIMBUS7-SMMR00000.dat is January 1979, NIMBUS7-SMMR00001.dat is February 1979, etc.\",\n    \"title\": \"Nimbus-7 SMMR global 60km gridded ocean parameters for 1979 - 1984\",\n    \"url\": \"ftp://podaac-ftp.jpl.nasa.gov/allData/nimbus7\"\n>>>>>>> datarefuge/master\n}\n\n\u0002wzxhzdk:1\u0003\n\nbagit.py --contact-name '[your name]' /directory/to/bag\n\n\u0002wzxhzdk:2\u0003\n\nbagit.py --contact-name '[your name]' /directory/to/bag\n\n\u0002wzxhzdk:3\u0003\n\nbagit.py --validate [directory/of/bag/to/validate]\n\n\u0002wzxhzdk:4\u0003\n\nbagit.py --validate [directory/of/bag/to/validate]\n``` < < < < < < <  HEAD\n- If it comes back as valid, open the bag and spot-check to make sure everything looks the same as when you uploaded it. (This will not affect the validity of the bags already uploaded). If all seems right, proceed to the rest of the quality assurance steps. If it does not come back as valid, make a note of the error, review your steps, and re-bag the file. If you continue to get invalid bags, please see a DataRescue guide or reach out to the #bagging Slack channel.\n- Fill out as much information as possible in the `Notes From Bagging` field in the Archivers app to document your work.\n- Check the checkbox that certifies this is a \"well-checked bag\"\n- Check the Bag checkbox (on the right-hand side) to mark that step as completed.\n- Click `Save`.\n- Click `Check in URL` to release it and allow someone else to work on the next step.\n=======\n- If it comes back as valid, open the bag and spot-check to make sure everything looks the same as when you uploaded it (this will not affect the validity of the bags already uploaded). If all seems right, proceed to the rest of the quality assurance steps. If it does not come back as valid, make a note of the error, review your steps, and re-bag the file. If you continue to get invalid bags, please see a DataRescue guide or reach out in the Baggers Slack channel.\n- Fill out as much information as possible in the `Notes From Bagging` field in the Archivers app to document your work.\n- Check the checkbox that certifies this is a \"well-checked bag\".\n- Check the Bag checkbox (far right on the same line as the \"Bag\" section heading) to mark that step as completed.\n- Click `Save`.\n- Click `Checkin this URL` to release it and allow someone else to work on the next step.\n>>>>>>> datarefuge/master", 
            "title": "Claiming a dataset for bagging"
        }, 
        {
            "location": "/describing/", 
            "text": "What Do Describers Do?\n\n\nDescribers create a descriptive record in the DataRefuge CKAN repository for each bag. Then they link the record to the bag and make the record public.\n\n\n\n  \nRecommended Skills\n \n  \n  Consider this path if you have experience working with scientific data (particularly climate or environmental data) or with metadata practices.\n\n\n\n\nGetting Set up as a Describer\n\n\n\n\nApply to become a Describer by asking your DataRescue guide or by filling out \nthis form\n.\n\n\nNote that an email address is required to apply.\n\n\nNote also that you should be willing to have your real name be associated with the datasets, to follow archival best practices (see \nguidelines on archival best practices for Data Refuge\n for more information).\n\n\n\n\n\n\nThe organizers of the event (in-person or remote) will send you an invite to the \nArchivers app\n, which helps us coordinate all the data archiving work we do.\n\n\nClick the invite link, and choose a user name and a password.\n\n\n\n\n\n\nCreate an account on the DataRefuge Slack using this \nslack-in\n (or use the Slack team recommended by your event organizers). This is where people share expertise and answer each other's questions.\n\n\nAsk your event organizer to send you an invite.\n\n\n\n\n\n\nThe organizers will also create an account for you in the CKAN instance at https://www.datarefuge.org/.\n\n\nTest that you can log in successfully.\n\n\n\n\n\n\nGet set up with Python and the Python script to make a bag at the command line: https://github.com/LibraryOfCongress/bagit-python\n\n\nIf you need any assistance:\n\n\nTalk to your DataRescue guide if you are at an in-person event.\n\n\nOr post questions on Slack in the Describers channel (or other channel recommended by your event organizers).\n\n\n\n\n\n\n\n\nClaiming a Bag\n\n\n\n\nYou will work on datasets that were bagged by Baggers.\n\n\nGo to the \nArchivers app\n, click \nURLS\n and then \nDESCRIBE\n: all the URLs listed are ready to be added to the CKAN instance.\n\n\nAvailable URLs are ones that have not been checked out by someone else, i.e. that do not have someone's name in the User column.\n\n\n\n\n\n\nSelect an available URL and click its UUID to get to the detailed view, then click \nCheckout this URL\n. It is now ready for you to work on, and no one else can do anything to it while you have it checked out.\n\n\n\n\n\n  \nNote: URL vs UUID\n \n  \n  The \nURL\n is the link to examine and harvest, and the \nUUID\n is a canonical ID we use to connect the URL with the data in question. The UUID will have been generated earlier in the process. UUID stands for Universal Unique Identifier.\n\n\n\n\nQA Step\n\n\n\n\nIn the Archivers app, scroll down to the \nDescribe\n section.\n\n\nThe URL of the zipped bag is in the \nBag Url / Location\n field.\n\n\nCut and paste that URL into your browser and download it.\n\n\nAfter downloading, unzip it.\n\n\nSpot-check some of the files (make sure they open and look normal, i.e., not garbled).\n\n\nIf the file fails QA:\n\n\nUncheck the Bagging checkbox.\n\n\nMake a note in the \nNotes From Bagging\n field, explaining in what way the bag failed QA and asking a bagger to please fix the issue.\n\n\n\n\n\n\n\n\nCreate New Record in CKAN\n\n\n\n\nGo to \nCKAN\n and click Organizations in the top menu.\n\n\nChoose the organization (i.e., federal agency) that your dataset belongs to, e.g. \nNOAA\n, and click it.\n\n\nIf the Organization you need does not exist yet, create it by clicking \nAdd Organization\n.\n\n\n\n\n\n\nClick \"Add Dataset\".\n\n\nStart entering metadata in the new record, following the metadata template below:\n\n\nTitle:\n Title of dataset, e.g., \"Form EIA-411 Data\".\n\n\nCustom Text: DO NOT Fill OUT (this field does not function properly at this time)\n\n\nDescription:\n Usually copied and pasted description found on webpage.\n\n\nTags:\n Basic descriptive keywords, e.g., \"electric reliability\", \"electricity\", \"power systems\".\n\n\nLicense:\n Choose value in dropdown. If there is no indicated license, select \"Other - Public Domain\".\n\n\nOrganization:\n Choose value in dropdown, e.g., \"United States Department of Energy\".\n\n\nVisibility:\n Select \"Public\".\n\n\nSource:\n URL where site is live, also in JSON, e.g. \"http://www.eia.gov/electricity/data/eia411/\".\n\n\n\n\n\n\nTo decide what value to enter in each field:\n\n\nOpen the JSON file that is in the bag you have downloaded; it contains some of the metadata you need.\n\n\nGo to the original location of the item on the federal agency website (found in the JSON file), to find more facts about the item such as description, title of the dataset, etc.\n\n\nAlternatively, you can also open the HTML file that should be included in the bag and is a copy of that original main page.\n\n\n\n\n\n\n\n\n\n\n\n\nEnhancing Existing Metadata\n\n\nThese sites will help you obtain federally-sourced metadata that can be added to the CKAN record for more accurate metadata:\n\n\n\n\nEPA:\n\n\nhttps://www.epa.gov/enviro/facility-registry-service-frs\n\n\nhttps://edg.epa.gov/metadata/catalog/main/home.page\n\n\n\n\n\n\n\n\nThese sites are sources of scientific metadata standards to review when choosing keywords:\n\n\n\n\nGCMD Keywords:\n\n\nhttps://wiki.earthdata.nasa.gov/display/cmr/gcmd+keyword+access\n - Downloadable CSV files of the GCMD taxonomies.\n\n\n\n\n\n\nATRAC:\n\n\nhttps://www.ncdc.noaa.gov/atrac/index.html\n - This is a free tool to give access to geographic metadata standards including autopopulating thesauri (GCMD and others commonly used with climate data).\n\n\n\n\n\n\n\n\nLinking the CKAN Record to the Bag\n\n\n\n\nClick \"Next: Add Data\" at the bottom of the CKAN form.\n\n\nEnter the following information:\n\n\nLink:\n Bag URL, e.g., \nhttps://drp-upload-bagger.s3.amazonaws.com/remote/77DD634E-EBCE-412E-88B5-A02B0EF12AF6_2.zip\n.\n\n\nName:\n filename, e.g., \n77DD634E-EBCE-412E-88B5-A02B0EF12AF6_2.zip\n.\n\n\nFormat:\n select \"Zip\".\n\n\n\n\n\n\nClick \"Finish\".\n\n\nTest that the link you just created works by clicking it, and verifying that the file begins to download.\n\n\nNote that you don't need to finish downloading it again.\n\n\nAlternatively, use WGET to test without downloading: \nwget --spider [BAG URL]\n\n\n\n\n\n\n\n\nAdding the CKAN record to the \"Data Rescue Events\" group\n\n\n\n\nOnce the record is created, click the tab \nGroups\n  \n\n\nSelect \nData Rescue Events\n in the dropdown and click \nAdd to Group\n.\n\n\nIn the future, it will be useful to be able to differentiate that among different groups of records based on how they were generated.\n\n\n\n\nFinishing Up\n\n\n\n\nIn the Archivers app, add the URL to the CKAN record in the \nCKAN URL\n field.\n\n\nThe syntax will be:\n \nhttps://www.datarefuge.org//dataset/[datasetNameGeneratedByCkan]\n\n\n\n\n\n\nAdd any useful notes to document your work.\n\n\nCheck the Describe checkbox (far right on the same line as the \"Describe\" section heading) to mark that step as completed.\n\n\nClick \nSave\n.\n\n\nClick \nCheckin this URL\n, to release it.\n\n\n\n\nPossible Tools: JSON Viewers\n\n\n\n\njsoneditoronline.org\n\n\njsonviewer.stack.hu", 
            "title": "Describing"
        }, 
        {
            "location": "/describing/#what-do-describers-do", 
            "text": "Describers create a descriptive record in the DataRefuge CKAN repository for each bag. Then they link the record to the bag and make the record public.  \n   Recommended Skills     \n  Consider this path if you have experience working with scientific data (particularly climate or environmental data) or with metadata practices.", 
            "title": "What Do Describers Do?"
        }, 
        {
            "location": "/describing/#getting-set-up-as-a-describer", 
            "text": "Apply to become a Describer by asking your DataRescue guide or by filling out  this form .  Note that an email address is required to apply.  Note also that you should be willing to have your real name be associated with the datasets, to follow archival best practices (see  guidelines on archival best practices for Data Refuge  for more information).    The organizers of the event (in-person or remote) will send you an invite to the  Archivers app , which helps us coordinate all the data archiving work we do.  Click the invite link, and choose a user name and a password.    Create an account on the DataRefuge Slack using this  slack-in  (or use the Slack team recommended by your event organizers). This is where people share expertise and answer each other's questions.  Ask your event organizer to send you an invite.    The organizers will also create an account for you in the CKAN instance at https://www.datarefuge.org/.  Test that you can log in successfully.    Get set up with Python and the Python script to make a bag at the command line: https://github.com/LibraryOfCongress/bagit-python  If you need any assistance:  Talk to your DataRescue guide if you are at an in-person event.  Or post questions on Slack in the Describers channel (or other channel recommended by your event organizers).", 
            "title": "Getting Set up as a Describer"
        }, 
        {
            "location": "/describing/#claiming-a-bag", 
            "text": "You will work on datasets that were bagged by Baggers.  Go to the  Archivers app , click  URLS  and then  DESCRIBE : all the URLs listed are ready to be added to the CKAN instance.  Available URLs are ones that have not been checked out by someone else, i.e. that do not have someone's name in the User column.    Select an available URL and click its UUID to get to the detailed view, then click  Checkout this URL . It is now ready for you to work on, and no one else can do anything to it while you have it checked out.   \n   Note: URL vs UUID     \n  The  URL  is the link to examine and harvest, and the  UUID  is a canonical ID we use to connect the URL with the data in question. The UUID will have been generated earlier in the process. UUID stands for Universal Unique Identifier.", 
            "title": "Claiming a Bag"
        }, 
        {
            "location": "/describing/#qa-step", 
            "text": "In the Archivers app, scroll down to the  Describe  section.  The URL of the zipped bag is in the  Bag Url / Location  field.  Cut and paste that URL into your browser and download it.  After downloading, unzip it.  Spot-check some of the files (make sure they open and look normal, i.e., not garbled).  If the file fails QA:  Uncheck the Bagging checkbox.  Make a note in the  Notes From Bagging  field, explaining in what way the bag failed QA and asking a bagger to please fix the issue.", 
            "title": "QA Step"
        }, 
        {
            "location": "/describing/#create-new-record-in-ckan", 
            "text": "Go to  CKAN  and click Organizations in the top menu.  Choose the organization (i.e., federal agency) that your dataset belongs to, e.g.  NOAA , and click it.  If the Organization you need does not exist yet, create it by clicking  Add Organization .    Click \"Add Dataset\".  Start entering metadata in the new record, following the metadata template below:  Title:  Title of dataset, e.g., \"Form EIA-411 Data\".  Custom Text: DO NOT Fill OUT (this field does not function properly at this time)  Description:  Usually copied and pasted description found on webpage.  Tags:  Basic descriptive keywords, e.g., \"electric reliability\", \"electricity\", \"power systems\".  License:  Choose value in dropdown. If there is no indicated license, select \"Other - Public Domain\".  Organization:  Choose value in dropdown, e.g., \"United States Department of Energy\".  Visibility:  Select \"Public\".  Source:  URL where site is live, also in JSON, e.g. \"http://www.eia.gov/electricity/data/eia411/\".    To decide what value to enter in each field:  Open the JSON file that is in the bag you have downloaded; it contains some of the metadata you need.  Go to the original location of the item on the federal agency website (found in the JSON file), to find more facts about the item such as description, title of the dataset, etc.  Alternatively, you can also open the HTML file that should be included in the bag and is a copy of that original main page.", 
            "title": "Create New Record in CKAN"
        }, 
        {
            "location": "/describing/#enhancing-existing-metadata", 
            "text": "These sites will help you obtain federally-sourced metadata that can be added to the CKAN record for more accurate metadata:   EPA:  https://www.epa.gov/enviro/facility-registry-service-frs  https://edg.epa.gov/metadata/catalog/main/home.page     These sites are sources of scientific metadata standards to review when choosing keywords:   GCMD Keywords:  https://wiki.earthdata.nasa.gov/display/cmr/gcmd+keyword+access  - Downloadable CSV files of the GCMD taxonomies.    ATRAC:  https://www.ncdc.noaa.gov/atrac/index.html  - This is a free tool to give access to geographic metadata standards including autopopulating thesauri (GCMD and others commonly used with climate data).", 
            "title": "Enhancing Existing Metadata"
        }, 
        {
            "location": "/describing/#linking-the-ckan-record-to-the-bag", 
            "text": "Click \"Next: Add Data\" at the bottom of the CKAN form.  Enter the following information:  Link:  Bag URL, e.g.,  https://drp-upload-bagger.s3.amazonaws.com/remote/77DD634E-EBCE-412E-88B5-A02B0EF12AF6_2.zip .  Name:  filename, e.g.,  77DD634E-EBCE-412E-88B5-A02B0EF12AF6_2.zip .  Format:  select \"Zip\".    Click \"Finish\".  Test that the link you just created works by clicking it, and verifying that the file begins to download.  Note that you don't need to finish downloading it again.  Alternatively, use WGET to test without downloading:  wget --spider [BAG URL]", 
            "title": "Linking the CKAN Record to the Bag"
        }, 
        {
            "location": "/describing/#adding-the-ckan-record-to-the-data-rescue-events-group", 
            "text": "Once the record is created, click the tab  Groups     Select  Data Rescue Events  in the dropdown and click  Add to Group .  In the future, it will be useful to be able to differentiate that among different groups of records based on how they were generated.", 
            "title": "Adding the CKAN record to the \"Data Rescue Events\" group"
        }, 
        {
            "location": "/describing/#finishing-up", 
            "text": "In the Archivers app, add the URL to the CKAN record in the  CKAN URL  field.  The syntax will be:\n  https://www.datarefuge.org//dataset/[datasetNameGeneratedByCkan]    Add any useful notes to document your work.  Check the Describe checkbox (far right on the same line as the \"Describe\" section heading) to mark that step as completed.  Click  Save .  Click  Checkin this URL , to release it.", 
            "title": "Finishing Up"
        }, 
        {
            "location": "/describing/#possible-tools-json-viewers", 
            "text": "jsoneditoronline.org  jsonviewer.stack.hu", 
            "title": "Possible Tools: JSON Viewers"
        }, 
        {
            "location": "/faq/", 
            "text": "The \nArchivers.space\n application is extremely fresh, we have some known issues and workarounds documented below.\n\n\n1) I'm looking at a URL, but I can't edit anything!\n\n\nMake sure you have clicked the big blue button \"\nCheckout this URL\n\" near the top. None of the fields can be edited until the URL is checked out.\n\n\n2) Why are URLs that may have already been archived by Ann Arbor available to research?\n\n\nWhen selecting a URL to review \"\n0\n\" is the \ndefault\n priority; it generally means that no-one has reviewed it \nUNLESS\n it says \nMAY HAVE BEEN HARVESTED AT ANN ARBOR\n.\n\n\nIn those cases, assign the priority to \"\n1\n\", so the URL drops down in the queue and then \nSKIP IT\n.\n\n\n3) What does it mean if it says \nCrawled by Internet Archive\n: Yes?\n\n\n\"\nCrawled by Internet Archive\n\" means the \npage itself\n was crawled; it may or may not mean the \ndataset\n was crawled.\n\n\nBased on what Heretrix \nCan and Can't Crawl\n, you will need to judge whether the dataset will be captured by the Internet Archive crawl and use your best judgement about whether to mark as \nDo not harvest\n.\n\n\n4) How should I handle overly broad sites with just a search form, e.g. noaa.gov?\n\n\nIn cases like \nnoaa.gov\n, you have to investigate  and try to find the data source a page is referencing and whether or not there is some way to query that data.\nIn many cases, it might be difficult or near impossible to isolate and query, depending on the kind of database.\n\n\nComplete the \nResearch\n section to the best of your abilities, especially the \nRecommended Approach for Harvesting Data\n.\n\n\n5) Do we have a scripting system set up preserving data or data endpoints that are updated regularly?\n\n\nNot yet; addressing these datasets is a goal going-forward.\n\n\nCurrently, indicate in the notes in both the \nResearch\n and \nHarvest\n sections that the dataset is updated regularly, and mark it complete anyway (note decision per @mattprice/this FAQ).\n\n\n6) What if I have a site and want to know if it has been crawled already?\n\n\nInternet Archive has both a \nWayback Machine Chrome Extension\n and \nAPIs\n you can use to check if something has been archived:\n\n\n\n\nWayback Machine Chrome Extension\n\n\nYou can also check on the Internet Archive site directly at \narchive.org/web/\n\n\nWayback CDX Server API\n\n\n\n\nThere is a \ncheck-ia\n script in the \nharvesting tools\n for batch URLs.\n\n\n7) What if the site has in fact been crawled well by the Internet Archive?\n\n\nIf the site includes only crawlable data, then there is no need to harvest it. These should be marked \nDo not harvest\n in the \nResearch\n phase.\n\n\nIf the site includes one of the forms of uncrawlable content:\n\n1) FTP\n\n2) Many Files\n\n3) Database\n\n4) Visualization/Interactive  \n\n\nThen mark accordingly in and harvest the datasets.\n\n\n8) What does it mean when it says \"checking out this url will unlock url: xxx\"?\n\n\nThat means you have another URL checked out. In order to avoid an overlap in efforts, when you check out a URL only you can work on it. By checking out a new URL the previous one will be unlocked.\n\n\n9) What do I do when there is stuff listed in the Link URL section?\n\n\nIf there are a bunch of sub-sites listed that are \nnot\n links, then you are on the master entry; the child entries are therefore just advisory and you should try to make sure that your harvesting includes all of the datasets contained across them, but otherwise keep going.\n\n\nIf the Link URL section has a single URL listed and it's a link, you are on a child item, which is the wrong place. Click the link and work on the master record.\n\n\n10) How do I partition large \"parent\" URLs (e.g., to reduce the size of the download \n 5 GB)?\n\n\nFrom the \noverview pane\n, click \nAdd Url\n on the top right side of app. Add a URL for each child and enter a description indicating these new URLs are children to the \"parent URL\". Make sure the priority of each child is the same as the parent.\n\n\nCheck out the parent URL, and under \nResearch\n use \"Link Url\" link it to all of its children and add a description. Make sure the priority of each child is the same as the parent. Start harvesting each child.\n\n\n11) Wifi is kind of Slow, are there workarounds for a faster connection?\n\n\n\n\n\n\nDo as much of the work as possible remotely: spin up a VM (e.g., AWS EC2, Digital Ocean droplet) or something, \nssh\n to those machines and do the downloading to there. The fewer people that are using the bandwidth onsite for big things, the less congestion this network will have.\n\n\n\n\n\n\nTether your phone :), thought if you do be mindful of bandwidth caps and don't forget to plug in your charger!\n\n\n\n\n\n\n12) Why can't I edit the harvesting section?\n\n\nArchivers is set up such that each URL moves through the stages of the workflow in sequence. In order to edit the \nHarvesting\n section, you will first need to mark \nResearch\n as complete. Look for the checkbox on the right-hand side at the top of the \nResearch\n section. Once you've checked it, make sure to hit \nSave\n.\n\n\n13) When harvesting, why doesn't clicking on the \nDownload Zipstarter\n button work?\n\n\nUnfortunately this is a known issue. Make sure you've marked \nResearch\n complete. Try reloading the page, or switching browsers if you can.\n\n\nThe App is not compatible with Safari.\n\n\n14) In the \nResearch\n section, what are all the checkboxes for?\n\n\nPlease read the DataRescue Workflow documentation for more info!\n\n\n15) I have a process improvement that would make this go better!\n\n\nGreat! Open an issue in your event's GitHub repository, or report it in the appropriate channel in your Slack team.\n\n\n16) How do I add a new event?\n\n\nAdmins can add events under the \"Events\" tab. Regular users will have to ask an admin for help!", 
            "title": "Archivers App FAQ"
        }, 
        {
            "location": "/faq/#1-im-looking-at-a-url-but-i-cant-edit-anything", 
            "text": "Make sure you have clicked the big blue button \" Checkout this URL \" near the top. None of the fields can be edited until the URL is checked out.", 
            "title": "1) I'm looking at a URL, but I can't edit anything!"
        }, 
        {
            "location": "/faq/#2-why-are-urls-that-may-have-already-been-archived-by-ann-arbor-available-to-research", 
            "text": "When selecting a URL to review \" 0 \" is the  default  priority; it generally means that no-one has reviewed it  UNLESS  it says  MAY HAVE BEEN HARVESTED AT ANN ARBOR .  In those cases, assign the priority to \" 1 \", so the URL drops down in the queue and then  SKIP IT .", 
            "title": "2) Why are URLs that may have already been archived by Ann Arbor available to research?"
        }, 
        {
            "location": "/faq/#3-what-does-it-mean-if-it-says-crawled-by-internet-archive-yes", 
            "text": "\" Crawled by Internet Archive \" means the  page itself  was crawled; it may or may not mean the  dataset  was crawled.  Based on what Heretrix  Can and Can't Crawl , you will need to judge whether the dataset will be captured by the Internet Archive crawl and use your best judgement about whether to mark as  Do not harvest .", 
            "title": "3) What does it mean if it says Crawled by Internet Archive: Yes?"
        }, 
        {
            "location": "/faq/#4-how-should-i-handle-overly-broad-sites-with-just-a-search-form-eg-noaagov", 
            "text": "In cases like  noaa.gov , you have to investigate  and try to find the data source a page is referencing and whether or not there is some way to query that data.\nIn many cases, it might be difficult or near impossible to isolate and query, depending on the kind of database.  Complete the  Research  section to the best of your abilities, especially the  Recommended Approach for Harvesting Data .", 
            "title": "4) How should I handle overly broad sites with just a search form, e.g. noaa.gov?"
        }, 
        {
            "location": "/faq/#5-do-we-have-a-scripting-system-set-up-preserving-data-or-data-endpoints-that-are-updated-regularly", 
            "text": "Not yet; addressing these datasets is a goal going-forward.  Currently, indicate in the notes in both the  Research  and  Harvest  sections that the dataset is updated regularly, and mark it complete anyway (note decision per @mattprice/this FAQ).", 
            "title": "5) Do we have a scripting system set up preserving data or data endpoints that are updated regularly?"
        }, 
        {
            "location": "/faq/#6-what-if-i-have-a-site-and-want-to-know-if-it-has-been-crawled-already", 
            "text": "Internet Archive has both a  Wayback Machine Chrome Extension  and  APIs  you can use to check if something has been archived:   Wayback Machine Chrome Extension  You can also check on the Internet Archive site directly at  archive.org/web/  Wayback CDX Server API   There is a  check-ia  script in the  harvesting tools  for batch URLs.", 
            "title": "6) What if I have a site and want to know if it has been crawled already?"
        }, 
        {
            "location": "/faq/#7-what-if-the-site-has-in-fact-been-crawled-well-by-the-internet-archive", 
            "text": "If the site includes only crawlable data, then there is no need to harvest it. These should be marked  Do not harvest  in the  Research  phase.  If the site includes one of the forms of uncrawlable content: \n1) FTP \n2) Many Files \n3) Database \n4) Visualization/Interactive    Then mark accordingly in and harvest the datasets.", 
            "title": "7) What if the site has in fact been crawled well by the Internet Archive?"
        }, 
        {
            "location": "/faq/#8-what-does-it-mean-when-it-says-checking-out-this-url-will-unlock-url-xxx", 
            "text": "That means you have another URL checked out. In order to avoid an overlap in efforts, when you check out a URL only you can work on it. By checking out a new URL the previous one will be unlocked.", 
            "title": "8) What does it mean when it says \"checking out this url will unlock url: xxx\"?"
        }, 
        {
            "location": "/faq/#9-what-do-i-do-when-there-is-stuff-listed-in-the-link-url-section", 
            "text": "If there are a bunch of sub-sites listed that are  not  links, then you are on the master entry; the child entries are therefore just advisory and you should try to make sure that your harvesting includes all of the datasets contained across them, but otherwise keep going.  If the Link URL section has a single URL listed and it's a link, you are on a child item, which is the wrong place. Click the link and work on the master record.", 
            "title": "9) What do I do when there is stuff listed in the Link URL section?"
        }, 
        {
            "location": "/faq/#10-how-do-i-partition-large-parent-urls-eg-to-reduce-the-size-of-the-download-5-gb", 
            "text": "From the  overview pane , click  Add Url  on the top right side of app. Add a URL for each child and enter a description indicating these new URLs are children to the \"parent URL\". Make sure the priority of each child is the same as the parent.  Check out the parent URL, and under  Research  use \"Link Url\" link it to all of its children and add a description. Make sure the priority of each child is the same as the parent. Start harvesting each child.", 
            "title": "10) How do I partition large \"parent\" URLs (e.g., to reduce the size of the download &lt; 5 GB)?"
        }, 
        {
            "location": "/faq/#11-wifi-is-kind-of-slow-are-there-workarounds-for-a-faster-connection", 
            "text": "Do as much of the work as possible remotely: spin up a VM (e.g., AWS EC2, Digital Ocean droplet) or something,  ssh  to those machines and do the downloading to there. The fewer people that are using the bandwidth onsite for big things, the less congestion this network will have.    Tether your phone :), thought if you do be mindful of bandwidth caps and don't forget to plug in your charger!", 
            "title": "11) Wifi is kind of Slow, are there workarounds for a faster connection?"
        }, 
        {
            "location": "/faq/#12-why-cant-i-edit-the-harvesting-section", 
            "text": "Archivers is set up such that each URL moves through the stages of the workflow in sequence. In order to edit the  Harvesting  section, you will first need to mark  Research  as complete. Look for the checkbox on the right-hand side at the top of the  Research  section. Once you've checked it, make sure to hit  Save .", 
            "title": "12) Why can't I edit the harvesting section?"
        }, 
        {
            "location": "/faq/#13-when-harvesting-why-doesnt-clicking-on-the-download-zipstarter-button-work", 
            "text": "Unfortunately this is a known issue. Make sure you've marked  Research  complete. Try reloading the page, or switching browsers if you can.  The App is not compatible with Safari.", 
            "title": "13) When harvesting, why doesn't clicking on the Download Zipstarter button work?"
        }, 
        {
            "location": "/faq/#14-in-the-research-section-what-are-all-the-checkboxes-for", 
            "text": "Please read the DataRescue Workflow documentation for more info!", 
            "title": "14) In the Research section, what are all the checkboxes for?"
        }, 
        {
            "location": "/faq/#15-i-have-a-process-improvement-that-would-make-this-go-better", 
            "text": "Great! Open an issue in your event's GitHub repository, or report it in the appropriate channel in your Slack team.", 
            "title": "15) I have a process improvement that would make this go better!"
        }, 
        {
            "location": "/faq/#16-how-do-i-add-a-new-event", 
            "text": "Admins can add events under the \"Events\" tab. Regular users will have to ask an admin for help!", 
            "title": "16) How do I add a new event?"
        }
    ]
}