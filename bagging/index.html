<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>Checking/Bagging - DataRescueNHV @ Yale Workflow</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../css/highlight.css">
  <link href="../css/extra.css" rel="stylesheet">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Checking/Bagging";
    var mkdocs_page_input_path = "bagging.md";
    var mkdocs_page_url = "/bagging/";
  </script>
  
  <script src="../js/jquery-2.1.1.min.js"></script>
  <script src="../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../js/highlight.pack.js"></script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> DataRescueNHV @ Yale Workflow</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="..">Home</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../logistics-procedures/">New Haven-Specific</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../self-sorting/">Which Path to Choose</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../advance-work/">Advance Work</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../seeding/">Seeding</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../researching/">Researching</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../harvesting/">Harvesting</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 current">
        <a class="current" href="./">Checking/Bagging</a>
        
            <ul>
            
                <li class="toctree-l3"><a href="#what-do-baggers-do">What do Baggers do?</a></li>
                
            
                <li class="toctree-l3"><a href="#baggers-do-some-quality-assurance-on-the-dataset-to-make-sure-the-content-is-correct-and-corresponds-to-what-was-described-in-the-spreadsheet-then-they-package-the-data-into-a-bagit-file-or-bag-which-includes-basic-technical-metadata-and-upload-it-to-final-datarefuge-destination">Baggers do some quality assurance on the dataset to make sure the content is correct and corresponds to what was described in the spreadsheet. Then they package the data into a bagit file (or "bag"), which includes basic technical metadata and upload it to final DataRefuge destination.</a></li>
                
                    <li><a class="toctree-l4" href="#what-do-baggers-do_1">What Do Baggers Do?</a></li>
                
                    <li><a class="toctree-l4" href="#getting-set-up-as-a-bagger">Getting set up as a Bagger</a></li>
                
                    <li><a class="toctree-l4" href="#claiming-a-dataset-for-bagging">Claiming a dataset for bagging</a></li>
                
            
            </ul>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../describing/">Describing</a>
        
    </li>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Additional Resources</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../faq/">Archivers App FAQ</a>
        
    </li>

        

        <li class="toctree-l1">
            <a class="" href="https://youtu.be/tvSSILnHnpA" target="_blank">Archivers App Walkthrough</a>
        </li>
        <li class="toctree-l1">
            <a class="" href="https://drive.google.com/file/d/0B8Gv3Zy5ceY3X0xiSlV2dnhMYWc/view" target="_blank">Crawlable or Uncrawlable Poster</a>
        </li>
    </ul>
<li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">DataRescueNHV @ Yale Workflow</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>Checking/Bagging</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD</p>
<h2 id="what-do-baggers-do">What do Baggers do?</h2>
<h1 id="baggers-do-some-quality-assurance-on-the-dataset-to-make-sure-the-content-is-correct-and-corresponds-to-what-was-described-in-the-spreadsheet-then-they-package-the-data-into-a-bagit-file-or-bag-which-includes-basic-technical-metadata-and-upload-it-to-final-datarefuge-destination">Baggers do some quality assurance on the dataset to make sure the content is correct and corresponds to what was described in the spreadsheet. Then they package the data into a bagit file (or "bag"), which includes basic technical metadata and upload it to final DataRefuge destination.</h1>
<h2 id="what-do-baggers-do_1">What Do Baggers Do?</h2>
<p>Baggers do some quality assurance on the dataset to make sure the content is correct and corresponds to what was described in the spreadsheet. Then they package the data into a bagit file (or "bag"), which includes basic technical metadata, and upload it to the final DataRefuge destination.</p>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>datarefuge/master</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<div class = "note">
  <strong>Recommended Skills</strong> <br />  
  Consider this path if you have data or web archiving experience, or have strong tech skills and an attention to detail.
</div>

<p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD</p>
<h2 id="getting-set-up-as-a-bagger">Getting set up as a Bagger</h2>
<ul>
<li>Skills recommended for this role: in general, Baggers need to have some tech skills and a good understanding of harvesting goals.</li>
<li>Apply to become a Bagger:<ul>
<li>By filling out <a href="https://docs.google.com/a/temple.edu/forms/d/e/1FAIpQLSfh9YIFnDrc-Cuc0hTd-U37J3D8xw8K7VXmzWkPs6Y5Q0wfVg/viewform">this form</a><ul>
<li>Note that an email address is required to apply.</li>
<li>Note also that you should be willing to have your real name be associated with the datasets, to follow archival best practices (see <a href="http://www.ppehlab.org/blogposts/2017/2/1/data-refuge-rests-on-a-clear-chain-of-custody">guidelines on archival best practices for Data Refuge</a> for more information).</li>
</ul>
</li>
</ul>
</li>
<li>The organizers of the event (in-person or remote) will send you an invite to the <a href="http://www.archivers.space/">Archivers app</a>, which helps us coordinate all the data archiving work we do.<ul>
<li>Click the invite link, and choose a user name and a password.</li>
</ul>
</li>
<li>Make sure you have an account on the DataRefuge slack (or other slack team recommended by your event organizers) This is where people share expertise and answer each other's questions.</li>
<li>Ask your event organizer to send you an invite</li>
<li>Get set up with Python and the Python script to make a bag at the command line<br />
<a href="https://github.com/LibraryOfCongress/bagit-python">https://github.com/LibraryOfCongress/bagit-python</a></li>
<li>If you need any assistance:</li>
<li>Talk to your DataRescue Guide if you are at an in-person event</li>
<li>Or post questions on Slack in the #Baggers channel(or other channel recommended by your event organizers).</li>
</ul>
<h2 id="claiming-a-dataset-for-bagging">Claiming a dataset for bagging</h2>
<ul>
<li>You will work on datasets that were harvested by Harvesters.</li>
<li>Go to the <a href="http://www.archivers.space/">Archivers app</a>, click <code>URLS</code> and then <code>BAG</code>: all the URLs listed are ready to be bagged<ul>
<li>Available URLs are the ones that have not been checked out by someone else, that is, that do not have someone's name in the User column.</li>
</ul>
</li>
<li>Select an available URL and click its UUID to get to the detailed view, then click <code>Check out this URL</code>. It is now ready for you to work on, and no one else can do anything to it while you have it checked out.</li>
<li>While you go through the bagging process, make sure to report as much information as possible in the Archivers app, as this is the place were we collectively keep track of all the work done.</li>
</ul>
<div class = "note">
  <strong>Note: URL vs UUID</strong> <br />  
  The <code>URL</code> is the link to examine and harvest, and the <code>UUID</code> is a canonical ID we use to connect the url with the data in question. The UUID will have been generated earlier earlier in the process. UUID stands for Universal Unique Identifier.
=======
**Note that the Checker role is currently performed by Baggers (See Quality Assurance step below). Eventually we would like the two to be separate, but the Archivers app does not offer that capability at this time.**

## Getting Set up as a Bagger

- Apply to become a Bagger by filling out [this form](https://docs.google.com/a/temple.edu/forms/d/e/1FAIpQLSfh9YIFnDrc-Cuc0hTd-U37J3D8xw8K7VXmzWkPs6Y5Q0wfVg/viewform)
    - Note that an email address is required to apply.
    - Note also that you should be willing to have your real name be associated with the datasets, to follow archival best practices (see [guidelines on archival best practices for Data Refuge](http://www.ppehlab.org/blogposts/2017/2/1/data-refuge-rests-on-a-clear-chain-of-custody) for more information).
- The organizers of the event (in-person or remote) will send you an invite to the [Archivers app](http://www.archivers.space/), which helps us coordinate all the data archiving work we do.
    - Click the invite link, and choose a user name and a password.
- Create an account on the DataRefuge Slack using this [slack-in](https://rauchg-slackin-qonsfhhvxs.now.sh/) (or use the Slack team recommended by your event organizers). This is where people share expertise and answer each other's questions.   
- Get set up with Python and the Python script to make a bag at the command line: [https://github.com/LibraryOfCongress/bagit-python](https://github.com/LibraryOfCongress/bagit-python)
- If you need any assistance:
     - Talk to your DataRescue guide if you are at an in-person event.
     - Or post questions on Slack in the #Baggers channel (or other channel recommended by your event organizers).

## Claiming a Dataset for Bagging

- You will work on datasets that were harvested by Harvesters.
- Go to the [Archivers app](http://www.archivers.space/), click `URLS` and then `BAG`: all the URLs listed are ready to be bagged.
    - Available URLs are the ones that have not been checked out by someone else, i.e. that do not have someone's name in the User column.
- Select an available URL and click its UUID to get to the detailed view, then click `Checkout this URL`. It is now ready for you to work on, and no one else can do anything to it while you have it checked out.
- While you go through the bagging process, make sure to report as much information as possible in the Archivers app, as this is the place where we collectively keep track of all the work done.

<div class = "note">
  <strong>Note: URL vs UUID</strong> <br />  
  The <code>URL</code> is the link to examine and harvest, and the <code>UUID</code> is a canonical ID we use to connect the URL with the data in question. The UUID will have been generated earlier in the process. UUID stands for Universal Unique Identifier.
>>>>>>> datarefuge/master
</div>

## Downloading & Opening the Dataset

<<<<<<< HEAD
- The zipped dataset that is ready to be bagged is under `Harvest Url / Location` in the the Archivers app. Download it to your laptop, and unzip it.
- Extra check: Is this URL truly ready to bag?
    - While everybody is doing their best to provide accurate information, occasionally a URL will be presented as "ready to bag", but, in fact, is not. Symptoms include:
    - There is no value in the "Harvest Url / Location" field
    - There is a note in the Harvest section that seem to indicate that the harvest was only partially performed.  
        - In either case, uncheck the "Harvest" checkbox, and add a note in the Harvest note, indicating that the URL does not seem ready for bagging and needs to be reviewed by a Harvester.
=======
- The zipped dataset that is ready to be bagged is under `Harvest Url / Location` in the the Archivers app. Download it to your laptop and unzip it.
- Extra check: Is this URL truly ready to bag?
    - While everybody is doing their best to provide accurate information, occasionally a URL will be presented as "ready to bag" but, in fact, is not. Symptoms include:
        - There is no value in the "Harvest Url / Location" field.
        - There is a note in the Harvest section that seems to indicate that the harvest was only partially performed.  
        - In either case, uncheck the "Harvest" checkbox, and add a note in the `Notes From Harvest` field indicating that the URL does not seem ready for bagging and needs to be reviewed by a Harvester.
>>>>>>> datarefuge/master

## Quality Assurance

- Confirm the harvested files:
    - Go to the original URL and check that the dataset is complete and accurate.
    - You also need to check that the dataset is meaningful, that is: "will the bag make sense to a scientist"?
    For instance, if a dataset is composed of a spreadsheet without any accompanying key or explanation of what the data represents, it might be completely impossible for a scientist to use it.
<<<<<<< HEAD
    - Spot check to make sure the files open properly and are not faulty in any way.
- Confirm contents of Json file:
    - The json should match the information from the Harvester and use the following format:  

wzxhzdk:0

{
    "Date of capture": "Fri Feb 24 2017 21:44:07 GMT-0800 (PST)",
    "File formats contained in package": ".xls, .zip",
    "Free text description of capture process": "Metadata was generated by viewing page, data was bulk downloaded using download_ftp_tree.py and then bagged.",
    "Individual source or seed URL": "ftp://podaac-ftp.jpl.nasa.gov/allData/nimbus7",
    "Institution facilitating the data capture creation and packaging": "DataRescue SF Bay",
    "Name of package creator": "JohnDoe",
    "Name of resource": "Nimbus-7 SMMR global 60km gridded ocean parameters for 1979 - 1984",
    "Type(s) of content in package": "These files are the Wentz Nimbus-7 SMMR global 60km gridded ocean parameters for 1979 - 1984.  There are 72 files which breaks down to 1 file per month. So, NIMBUS7-SMMR00000.dat is January 1979, NIMBUS7-SMMR00001.dat is February 1979, etc.",
    "UUID": "F3499E3B-7517-4C06-A661-72B4DA13A2A2",
    "recommended_approach": "",
    "significance": "These files are the Wentz Nimbus-7 SMMR global 60km gridded ocean parameters for 1979 - 1984.  There are 72 files which breaks down to 1 file per month. So, NIMBUS7-SMMR00000.dat is January 1979, NIMBUS7-SMMR00001.dat is February 1979, etc.",
    "title": "Nimbus-7 SMMR global 60km gridded ocean parameters for 1979 - 1984",
    "url": "ftp://podaac-ftp.jpl.nasa.gov/allData/nimbus7"
>>>>>>> datarefuge/master
}

wzxhzdk:1

bagit.py --contact-name '[your name]' /directory/to/bag

wzxhzdk:2

bagit.py --contact-name '[your name]' /directory/to/bag

wzxhzdk:3

bagit.py --validate [directory/of/bag/to/validate]

wzxhzdk:4

bagit.py --validate [directory/of/bag/to/validate]
```

<<<<<<< HEAD
- If it comes back as valid, open the bag and spot-check to make sure everything looks the same as when you uploaded it. (This will not affect the validity of the bags already uploaded). If all seems right, proceed to the rest of the quality assurance steps. If it does not come back as valid, make a note of the error, review your steps, and re-bag the file. If you continue to get invalid bags, please see a DataRescue guide or reach out to the #bagging Slack channel.
- Fill out as much information as possible in the `Notes From Bagging` field in the Archivers app to document your work.
- Check the checkbox that certifies this is a "well-checked bag"
- Check the Bag checkbox (on the right-hand side) to mark that step as completed.
- Click `Save`.
- Click `Check in URL` to release it and allow someone else to work on the next step.
=======
- If it comes back as valid, open the bag and spot-check to make sure everything looks the same as when you uploaded it (this will not affect the validity of the bags already uploaded). If all seems right, proceed to the rest of the quality assurance steps. If it does not come back as valid, make a note of the error, review your steps, and re-bag the file. If you continue to get invalid bags, please see a DataRescue guide or reach out in the Baggers Slack channel.
- Fill out as much information as possible in the `Notes From Bagging` field in the Archivers app to document your work.
- Check the checkbox that certifies this is a "well-checked bag".
- Check the Bag checkbox (far right on the same line as the "Bag" section heading) to mark that step as completed.
- Click `Save`.
- Click `Checkin this URL` to release it and allow someone else to work on the next step.
>>>>>>> datarefuge/master
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../describing/" class="btn btn-neutral float-right" title="Describing">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../harvesting/" class="btn btn-neutral" title="Harvesting"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    <p>This documentation is licensed under a <a href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.</p>
  </div>

  <p>Source available at <a href="https://github.com/datarefuge/workflow" target="_blank"><i class="fa fa-github"></i>&nbsp;datarefuge/workflow</a>, built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.</p>
</footer>
	  
        </div>
      </div>

    </section>
    
  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../harvesting/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../describing/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script src="../js/theme.js"></script>

</body>
</html>
